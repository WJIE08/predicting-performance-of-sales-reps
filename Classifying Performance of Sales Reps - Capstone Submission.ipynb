{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Moving the Bottom Line Needle: Classifying Performance of Sales Representatives in a Technology Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student: Tan Wei Jie\n",
    "### Academic Supervisor: Mr Herman Tan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import datetime as dt\n",
    "import re\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error, explained_variance_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.compose import  ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, CategoricalNB\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from itertools import cycle\n",
    "\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change directory to required folder\n",
    "os.chdir('C:/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the FY21 sales attainment data\n",
    "df = pd.read_csv('./data/FY21 Classifying Performance of Sales Reps - Sample Dataset v2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872, 36)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Step 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "\n",
    "df = df.rename({'# of Salary Change FY20': 'NUM_SALARY_CHANGE_PREV_FY',\n",
    "               '# of Salary Change FY21': 'NUM_SALARY_CHANGE_CURR_FY',\n",
    "               '# of Org Change FY20': 'NUM_ORG_CHANGE_PREV_FY', \n",
    "                '# of Org Change FY21':'NUM_ORG_CHANGE_CURR_FY',\n",
    "                '# of Career Level Change FY20':'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
    "                '# of Career Level Change FY21':'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
    "               '# of Manager Change FY20': 'NUM_MANAGER_CHANGE_PREV_FY',\n",
    "               '# of Manager Change FY21': 'NUM_MANAGER_CHANGE_CURR_FY',\n",
    "               '# of Job Change FY20':'NUM_JOB_CHANGE_PREV_FY',\n",
    "                '# of Job Change FY21':'NUM_JOB_CHANGE_CURR_FY',\n",
    "                '# of Location Change FY20':'NUM_LOCATION_CHANGE_PREV_FY',\n",
    "                '# of Location Change FY21':'NUM_LOCATION_CHANGE_CURR_FY',\n",
    "                'ATTAINMENT_REP_LEVEL_FY20':'ATTAINMENT_REP_LEVEL_PREV_FY',\n",
    "               'MANAGER180_OVERALL':'MANAGER180_OVERALL_CURR_FY'}, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all column headers, remove white spaces, remove punctuation and convert to uppercase\n",
    "df.columns = df.columns.str.replace('\\s', '_', regex=True).str.replace('[^\\w\\s]', '', regex=True).str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any duplicates\n",
    "#df_check = df[['EMAIL_ADDRESS']]\n",
    "\n",
    "# df_dup = df_check[df_check.duplicated(subset=['EMAIL_ADDRESS'], keep=False)]\n",
    "# df_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round attainment values to 2 decimal places\n",
    "df1['ATTAINMENT_REP_LEVEL_FY21'] = round(df1['ATTAINMENT_REP_LEVEL_FY21'],2)\n",
    "df1['ATTAINMENT_REP_LEVEL_PREV_FY'] = round(df1['ATTAINMENT_REP_LEVEL_PREV_FY'],2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#check for null values in attainment\n",
    "print(df1['ATTAINMENT_REP_LEVEL_FY21'].isnull().sum())\n",
    "print(df1['ATTAINMENT_REP_LEVEL_PREV_FY'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNNAMED_0</th>\n",
       "      <th>SUBREGION</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>CAREER_LEVEL</th>\n",
       "      <th>TENURE_CONTINUOUS_SERVICE_DATE_BAND</th>\n",
       "      <th>TENURE_LATEST_HIRE_DATE_BAND</th>\n",
       "      <th>JOB_TENURE_BAND</th>\n",
       "      <th>TENURE_CONTINUOUS_SERVICE_DATE_IN_YEARS</th>\n",
       "      <th>TENURE_LATEST_SERVICE_DATE_IN_YEARS</th>\n",
       "      <th>JOB_TENURE_IN_YEARS</th>\n",
       "      <th>TIME_SINCE_LAST_SALARY_INCR_BAND</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE_BAND</th>\n",
       "      <th>PRODUCT_LINE</th>\n",
       "      <th>PRODUCT_ASSOCIATION</th>\n",
       "      <th>RCODE_06</th>\n",
       "      <th>RCODE_07</th>\n",
       "      <th>ATTAINMENT_REP_LEVEL_FY21</th>\n",
       "      <th>ATTAINMENT_REP_LEVEL_PREV_FY</th>\n",
       "      <th>MANAGER180_OVERALL_CURR_FY</th>\n",
       "      <th>NUM_SALARY_CHANGE_PREV_FY</th>\n",
       "      <th>NUM_SALARY_CHANGE_CURR_FY</th>\n",
       "      <th>NUM_ORG_CHANGE_PREV_FY</th>\n",
       "      <th>NUM_ORG_CHANGE_CURR_FY</th>\n",
       "      <th>NUM_CAREER_LEVEL_CHANGE_PREV_FY</th>\n",
       "      <th>NUM_CAREER_LEVEL_CHANGE_CURR_FY</th>\n",
       "      <th>NUM_MANAGER_CHANGE_PREV_FY</th>\n",
       "      <th>NUM_MANAGER_CHANGE_CURR_FY</th>\n",
       "      <th>NUM_JOB_CHANGE_PREV_FY</th>\n",
       "      <th>NUM_JOB_CHANGE_CURR_FY</th>\n",
       "      <th>NUM_LOCATION_CHANGE_PREV_FY</th>\n",
       "      <th>NUM_LOCATION_CHANGE_CURR_FY</th>\n",
       "      <th>NATIONALITY</th>\n",
       "      <th>MANAGER_GENDER_DESC</th>\n",
       "      <th>HIRE_EVENT_DESCRIPTION</th>\n",
       "      <th>MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [UNNAMED_0, SUBREGION, COUNTRY, CAREER_LEVEL, TENURE_CONTINUOUS_SERVICE_DATE_BAND, TENURE_LATEST_HIRE_DATE_BAND, JOB_TENURE_BAND, TENURE_CONTINUOUS_SERVICE_DATE_IN_YEARS, TENURE_LATEST_SERVICE_DATE_IN_YEARS, JOB_TENURE_IN_YEARS, TIME_SINCE_LAST_SALARY_INCR_BAND, GENDER, AGE_BAND, PRODUCT_LINE, PRODUCT_ASSOCIATION, RCODE_06, RCODE_07, ATTAINMENT_REP_LEVEL_FY21, ATTAINMENT_REP_LEVEL_PREV_FY, MANAGER180_OVERALL_CURR_FY, NUM_SALARY_CHANGE_PREV_FY, NUM_SALARY_CHANGE_CURR_FY, NUM_ORG_CHANGE_PREV_FY, NUM_ORG_CHANGE_CURR_FY, NUM_CAREER_LEVEL_CHANGE_PREV_FY, NUM_CAREER_LEVEL_CHANGE_CURR_FY, NUM_MANAGER_CHANGE_PREV_FY, NUM_MANAGER_CHANGE_CURR_FY, NUM_JOB_CHANGE_PREV_FY, NUM_JOB_CHANGE_CURR_FY, NUM_LOCATION_CHANGE_PREV_FY, NUM_LOCATION_CHANGE_CURR_FY, NATIONALITY, MANAGER_GENDER_DESC, HIRE_EVENT_DESCRIPTION, MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if there are any attainment values that are negative\n",
    "df1.loc[df1['ATTAINMENT_REP_LEVEL_PREV_FY'] < 0.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Step 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: EDA charts are based on fictitious data and do not reflect original values**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a histogram to show the distribution of the attainment percentage\n",
    "plt.figure(figsize = (12,9))\n",
    "\n",
    "sns.distplot(df1['ATTAINMENT_REP_LEVEL_FY21'], kde=False, color='blue', bins=10)\n",
    "\n",
    "plt.title('Distribution of Attainment Rep Level for FY21', fontsize=18)\n",
    "plt.xlabel('Attainment in %)', fontsize=16)\n",
    "plt.ylabel('Counts', fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts\n",
    "df1['ATTAINMENT_REP_LEVEL_FY21'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the attainment values into multiclass categories\n",
    "#bins were created based on business definition of good and poor performance\n",
    "\n",
    "bins = [-1, 25, 75, 100, float(\"inf\")]\n",
    "\n",
    "bands = ['Poor','Average','Good','Great']\n",
    "\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_MULTICLASS_FY21'] = pd.cut(df1['ATTAINMENT_REP_LEVEL_FY21'], bins, labels=bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_MULTICLASS_FY21'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Values below 75 are 'Not Good' while values above 75 are 'Good'**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the attainment values into binary categories\n",
    "#bins were created based on business definition of good and poor performance\n",
    "\n",
    "bins = [-1, 75, float(\"inf\")]\n",
    "\n",
    "bands = ['Not Good', 'Good']\n",
    "\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_FY21'] = pd.cut(df1['ATTAINMENT_REP_LEVEL_FY21'], bins, labels=bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the attainment values into binary categories\n",
    "#bins were created based on business definition of good and poor performance\n",
    "\n",
    "bins = [-1, 75, float(\"inf\")]\n",
    "\n",
    "bands = [0, 1]\n",
    "\n",
    "df1['ATTAINMENT_REP_LEVEL_BANDNUM_FY21'] = pd.cut(df1['ATTAINMENT_REP_LEVEL_FY21'], bins, labels=bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_FY21'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts\n",
    "df1['ATTAINMENT_REP_LEVEL_BANDNUM_FY21'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise the outcome counts after banding\n",
    "plt.figure(figsize = (7,7))\n",
    "\n",
    "sns.countplot(x = df1['ATTAINMENT_REP_LEVEL_BAND_FY21'], data = df1, palette='Blues')\n",
    "\n",
    "plt.title('Distribution of Attainment Rep Level for FY21', fontsize=18)\n",
    "plt.xlabel('Attainment Banding)', fontsize=16)\n",
    "plt.ylabel('Counts', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot demographic variables against binary outcome\n",
    "subset1 = df1[['GENDER','AGE_BAND','ATTAINMENT_REP_LEVEL_BAND_FY21']]\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize = (20,10), facecolor='white')\n",
    "fig.suptitle('Count of demographic variables by outcome')\n",
    "\n",
    "ax1 = sns.countplot(x='GENDER', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset1, palette='Blues', ax=axes[0])\n",
    "ax2 = sns.countplot(x='AGE_BAND', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset1, palette='Blues', ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot HR related variables against binary outcome\n",
    "subset2 = df1[['SUBREGION','CAREER_LEVEL','TENURE_CONTINUOUS_SERVICE_DATE_BAND',\n",
    "               'TENURE_LATEST_HIRE_DATE_BAND','JOB_TENURE_BAND','TIME_SINCE_LAST_SALARY_INCR_BAND',\n",
    "               'HIRE_EVENT_DESCRIPTION','ATTAINMENT_REP_LEVEL_BAND_FY21']]\n",
    "\n",
    "fig, axes = plt.subplots(4,2, figsize = (20,20), facecolor='white')\n",
    "fig.suptitle('Count of career variables by outcome')\n",
    "\n",
    "ax1 = sns.countplot(x='SUBREGION', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset2, palette='Blues', ax=axes[0,0])\n",
    "ax2 = sns.countplot(x='CAREER_LEVEL', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset2, palette='Blues', ax=axes[0,1])\n",
    "ax3 = sns.countplot(x='TENURE_CONTINUOUS_SERVICE_DATE_BAND', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset2, palette='Blues', ax=axes[1,0])\n",
    "ax4 = sns.countplot(x='TENURE_LATEST_HIRE_DATE_BAND', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset2, palette='Blues', ax=axes[1,1])\n",
    "ax5 = sns.countplot(x='JOB_TENURE_BAND', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset2, palette='Blues', ax=axes[2,0])\n",
    "ax6 = sns.countplot(x='TIME_SINCE_LAST_SALARY_INCR_BAND', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset2, palette='Blues', ax=axes[2,1])\n",
    "ax7 = sns.countplot(x='HIRE_EVENT_DESCRIPTION', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', data=subset2, palette='Blues', ax=axes[3,0])\n",
    "\n",
    "\n",
    "# f.delaxes(ax=axes[2,1])\n",
    "# f.delaxes(ax=axes[2,2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot distribution of previous financial year Attainment and manager perception survey scores\n",
    "subset4 = df1[['MANAGER180_OVERALL_CURR_FY','ATTAINMENT_REP_LEVEL_BAND_FY21','ATTAINMENT_REP_LEVEL_PREV_FY']]\n",
    "subset5 = df1[['ATTAINMENT_REP_LEVEL_PREV_FY','ATTAINMENT_REP_LEVEL_BAND_FY21']]\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize = (20,10), facecolor='white')\n",
    "\n",
    "sns.histplot(data=subset4, x='MANAGER180_OVERALL_CURR_FY' , bins=25, ax=axes[0])\n",
    "\n",
    "sns.histplot(data=subset5, x='ATTAINMENT_REP_LEVEL_PREV_FY',bins=25, ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the median of the current year performance and mean of the manager perception survey scores\n",
    "from numpy import median,mean\n",
    "subset6 = df1[['MANAGER180_OVERALL_CURR_FY','ATTAINMENT_REP_LEVEL_PREV_FY','ATTAINMENT_REP_LEVEL_BAND_FY21']]\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize = (20,10), facecolor='white')\n",
    "\n",
    "sns.barplot(data=subset6, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y=\"ATTAINMENT_REP_LEVEL_PREV_FY\", estimator=median, \n",
    "            palette='Blues',ci=None, ax=axes[0])\n",
    "sns.barplot(data=subset6, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y=\"MANAGER180_OVERALL_CURR_FY\", estimator=mean, \n",
    "            palette='Blues', ci=None, ax=axes[1])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairwise plots\n",
    "\n",
    "subset9 = df1[['MANAGER180_OVERALL_CURR_FY','ATTAINMENT_REP_LEVEL_PREV_FY','ATTAINMENT_REP_LEVEL_BAND_FY21']]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,8), dpi= 100)\n",
    "\n",
    "#sns.pairplot(subset6, kind='reg', hue='ATTAINMENT_REP_LEVEL_BAND_FY21', plot_kws=dict(s=80,edgecolor=\"white\", linewidth=2.5))\n",
    "\n",
    "sns.pairplot(subset9, kind=\"reg\", hue='ATTAINMENT_REP_LEVEL_BAND_FY21')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create barplots with mean values for previous financial year numerical variables\n",
    "\n",
    "subset7 = df1[['NUM_SALARY_CHANGE_PREV_FY','NUM_ORG_CHANGE_PREV_FY','NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
    "               'NUM_MANAGER_CHANGE_PREV_FY','NUM_JOB_CHANGE_PREV_FY','NUM_LOCATION_CHANGE_PREV_FY',\n",
    "               'ATTAINMENT_REP_LEVEL_BAND_FY21']]\n",
    "\n",
    "fig, axes = plt.subplots(3,2, figsize = (20,20), facecolor='white')\n",
    "\n",
    "sns.barplot(data=subset7, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_SALARY_CHANGE_PREV_FY', \n",
    "            palette='Blues',ci=None, ax=axes[0,0])\n",
    "sns.barplot(data=subset7, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_ORG_CHANGE_PREV_FY', \n",
    "            palette='Blues', ci=None, ax=axes[0,1])\n",
    "sns.barplot(data=subset7, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_CAREER_LEVEL_CHANGE_PREV_FY', \n",
    "            palette='Blues', ci=None, ax=axes[1,0])\n",
    "sns.barplot(data=subset7, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_MANAGER_CHANGE_PREV_FY', \n",
    "            palette='Blues', ci=None, ax=axes[1,1])\n",
    "sns.barplot(data=subset7, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_JOB_CHANGE_PREV_FY', \n",
    "            palette='Blues', ci=None, ax=axes[2,0])\n",
    "sns.barplot(data=subset7, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_LOCATION_CHANGE_PREV_FY', \n",
    "            palette='Blues', ci=None, ax=axes[2,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create barplots with mean values for current financial year numerical variables\n",
    "\n",
    "subset8 = df1[['NUM_SALARY_CHANGE_CURR_FY','NUM_ORG_CHANGE_CURR_FY','NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
    "               'NUM_MANAGER_CHANGE_CURR_FY','NUM_JOB_CHANGE_CURR_FY','NUM_LOCATION_CHANGE_CURR_FY',\n",
    "               'ATTAINMENT_REP_LEVEL_BAND_FY21']]\n",
    "\n",
    "fig, axes = plt.subplots(3,2, figsize = (20,20), facecolor='white')\n",
    "\n",
    "\n",
    "sns.barplot(data=subset8, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_SALARY_CHANGE_CURR_FY', \n",
    "            palette='Blues', ci=None, ax=axes[0,0])\n",
    "sns.barplot(data=subset8, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_ORG_CHANGE_CURR_FY', \n",
    "            palette='Blues', ci=None, ax=axes[0,1])\n",
    "sns.barplot(data=subset8, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_CAREER_LEVEL_CHANGE_CURR_FY', \n",
    "            palette='Blues', ci=None, ax=axes[1,0])\n",
    "sns.barplot(data=subset8, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_MANAGER_CHANGE_CURR_FY', \n",
    "            palette='Blues', ci=None, ax=axes[1,1])\n",
    "sns.barplot(data=subset8, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_JOB_CHANGE_CURR_FY', \n",
    "            palette='Blues', ci=None, ax=axes[2,0])\n",
    "sns.barplot(data=subset8, x='ATTAINMENT_REP_LEVEL_BAND_FY21',y='NUM_LOCATION_CHANGE_CURR_FY', \n",
    "            palette='Blues', ci=None, ax=axes[2,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Step 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3.1 Manager Perception Scores: Binning and recoding**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of missing values for current year manager perception survey\n",
    "df1['MANAGER180_OVERALL_CURR_FY'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the distribution of the manager perception survey scores\n",
    "df1['MANAGER180_OVERALL_CURR_FY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the manager perception survey scores\n",
    "bins = [-1, 25, 50, 75, float(\"inf\")]\n",
    "\n",
    "bands = ['Low', 'Avg', 'High', 'Very High']\n",
    "\n",
    "df1['MANAGER180_OVERALL_BAND_CURR_FY'] = pd.cut(df1['MANAGER180_OVERALL_CURR_FY'], bins, labels=bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data type back to string and label missing values\n",
    "df1['MANAGER180_OVERALL_BAND_CURR_FY'] = df1['MANAGER180_OVERALL_BAND_CURR_FY'].astype(str)\n",
    "df1['FY21_MANAGER180_OVERALL_CURR_FY'] = df1['MANAGER180_OVERALL_BAND_CURR_FY'].fillna('Missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the distribution of the manager perception survey scores\n",
    "df1['MANAGER180_OVERALL_BAND_CURR_FY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3.2 Previous Financial Year Performance: Binning and recoding**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for number of missing values in pervious financial year performance\n",
    "df1['ATTAINMENT_REP_LEVEL_PREV_FY'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the previous financial year attainment values into categorical string\n",
    "\n",
    "bins = [-1, 25, 75, 100, float(\"inf\")]\n",
    "\n",
    "bands = ['Poor', 'Avg', 'Good','Great']\n",
    "\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_PREV_FY'] = pd.cut(df1['ATTAINMENT_REP_LEVEL_PREV_FY'], bins, labels=bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data type back to string and label missing values\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_PREV_FY'] = df1['ATTAINMENT_REP_LEVEL_BAND_PREV_FY'].astype(str)\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_PREV_FY'] = df1['ATTAINMENT_REP_LEVEL_BAND_PREV_FY'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the distribution of the previous financial year banding\n",
    "df1['ATTAINMENT_REP_LEVEL_BAND_PREV_FY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3.3 Numerical Variables: Binning and recoding**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the tenure and job date values into categorical string\n",
    "\n",
    "bins = [-1, 0.999, 1.999, 2.999, 3.999, 4.999, 9.999, 19.999, float(\"inf\")]\n",
    "\n",
    "bands = ['0 to 1 yr', '1 to 2 yrs', '2 to 3 yrs','3 to 4 yrs', '4 to 5 yrs', '5 to 10 yrs', '10 to 20 yrs', 'above 20 yrs']\n",
    "\n",
    "df1['TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN'] = pd.cut(df1['TENURE_CONTINUOUS_SERVICE_DATE_IN_YEARS'], bins, labels=bands)\n",
    "\n",
    "df1['TENURE_LATEST_HIRE_DATE_BAND_LIN'] = pd.cut(df1['TENURE_LATEST_SERVICE_DATE_IN_YEARS'], bins, labels=bands)\n",
    "\n",
    "df1['JOB_TENURE_BAND_LIN'] = pd.cut(df1['JOB_TENURE_IN_YEARS'], bins, labels=bands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3.4 Categorical Variables: Recoding**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts of hire event type\n",
    "df1['HIRE_EVENT_DESCRIPTION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode hire event type\n",
    "df1['HIRE_EVENT_DESCRIPTION'] = df1['HIRE_EVENT_DESCRIPTION'].replace(0, 'Unspecified', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts of product line\n",
    "df1['PRODUCT_LINE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode the values with count of less than 10 to 'Others'\n",
    "\n",
    "df1.loc[df1.groupby('PRODUCT_LINE')['PRODUCT_LINE'].transform('count').lt(10), 'PRODUCT_LINE'] = 'Others'    \n",
    "df1['PRODUCT_LINE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts of product association\n",
    "df1['PRODUCT_ASSOCIATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode the values with count of less than 13 to 'Others'\n",
    "\n",
    "df1.loc[df1.groupby('PRODUCT_ASSOCIATION')['PRODUCT_ASSOCIATION'].transform('count').lt(13), 'PRODUCT_ASSOCIATION'] = 'Others'    \n",
    "df1['PRODUCT_ASSOCIATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts of Rcode 06\n",
    "df1['RCODE_06'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode the values with count of less than 10 to 'Others'\n",
    "\n",
    "df1.loc[df1.groupby('RCODE_06')['RCODE_06'].transform('count').lt(10), 'RCODE_06'] = 'Others'    \n",
    "df1['RCODE_06'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts of Rcode 07\n",
    "df1['RCODE_07'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode the values with count of less than 8 to 'Others'\n",
    "df1.loc[df1.groupby('RCODE_07')['RCODE_07'].transform('count').lt(8), 'RCODE_07'] = 'Others'    \n",
    "df1['RCODE_07'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the categorical columns to string\n",
    "\n",
    "col_list = list(df1.select_dtypes(include=['category']))\n",
    "\n",
    "\n",
    "df1[col_list] = df1[col_list].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Step 4.1 Classification Without Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.1.1 Train-Test-Validation Split**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of the previous dataframe\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Not Good', 'Good'], dtype=object)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['ATTAINMENT_REP_LEVEL_BAND_FY21'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts of outcome\n",
    "df2['ATTAINMENT_REP_LEVEL_BAND_FY21'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure that the outcome dtype is string and integer respectively\n",
    "df2['ATTAINMENT_REP_LEVEL_BAND_FY21'] = df2['ATTAINMENT_REP_LEVEL_BAND_FY21'].astype(str)\n",
    "df2['ATTAINMENT_REP_LEVEL_BANDNUM_FY21'] = df2['ATTAINMENT_REP_LEVEL_BANDNUM_FY21'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert categorical columns to string\n",
    "\n",
    "cat_cols = ['SUBREGION','COUNTRY','CAREER_LEVEL','TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
    "            'TENURE_LATEST_HIRE_DATE_BAND_LIN','JOB_TENURE_BAND_LIN','TIME_SINCE_LAST_SALARY_INCR_BAND',\n",
    "            'GENDER','AGE_BAND','PRODUCT_LINE','PRODUCT_ASSOCIATION','RCODE_06','RCODE_07','NATIONALITY',\n",
    "            'MANAGER_GENDER_DESC','HIRE_EVENT_DESCRIPTION',\n",
    "            'MANAGER180_OVERALL_BAND_CURR_FY','ATTAINMENT_REP_LEVEL_BAND_PREV_FY','MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL']\n",
    "\n",
    "#convert categorical columns to string\n",
    "\n",
    "df2[cat_cols] = df2[cat_cols].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the feature columns\n",
    "feature_cols = ['SUBREGION','COUNTRY','CAREER_LEVEL','TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
    "                'TENURE_LATEST_HIRE_DATE_BAND_LIN','JOB_TENURE_BAND_LIN','TIME_SINCE_LAST_SALARY_INCR_BAND',\n",
    "                'GENDER','AGE_BAND','PRODUCT_LINE','PRODUCT_ASSOCIATION','RCODE_06','RCODE_07',\n",
    "                'NATIONALITY','MANAGER_GENDER_DESC','HIRE_EVENT_DESCRIPTION',\n",
    "                'NUM_SALARY_CHANGE_PREV_FY','NUM_SALARY_CHANGE_CURR_FY',\n",
    "                'NUM_ORG_CHANGE_PREV_FY','NUM_ORG_CHANGE_CURR_FY','NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
    "                'NUM_CAREER_LEVEL_CHANGE_CURR_FY','NUM_MANAGER_CHANGE_PREV_FY','NUM_MANAGER_CHANGE_CURR_FY',\n",
    "                'NUM_JOB_CHANGE_PREV_FY','NUM_JOB_CHANGE_CURR_FY','NUM_LOCATION_CHANGE_PREV_FY',\n",
    "                'NUM_LOCATION_CHANGE_CURR_FY','MANAGER180_OVERALL_BAND_CURR_FY','ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
    "                'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL']\n",
    "\n",
    "#define X and y\n",
    "\n",
    "X = df2[feature_cols]\n",
    "y = df2['ATTAINMENT_REP_LEVEL_BANDNUM_FY21']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts for y\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872, 31)\n",
      "(872,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the data into 60% training data, 20% validation data and 20% testing data\n",
    "# validation dataset is used to ensure that the model does not see the whole dataset\n",
    "\n",
    "#split the dataset into training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "#split the training data again into training data and validation data. \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=32) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.1.2 Naive Classifier**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define the numeric features\n",
    "numeric_features = ['NUM_SALARY_CHANGE_PREV_FY','NUM_SALARY_CHANGE_CURR_FY','NUM_ORG_CHANGE_PREV_FY','NUM_ORG_CHANGE_CURR_FY',\n",
    "                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY','NUM_CAREER_LEVEL_CHANGE_CURR_FY','NUM_MANAGER_CHANGE_PREV_FY',\n",
    "                    'NUM_MANAGER_CHANGE_CURR_FY','NUM_JOB_CHANGE_PREV_FY','NUM_JOB_CHANGE_CURR_FY',\n",
    "                    'NUM_LOCATION_CHANGE_PREV_FY','NUM_LOCATION_CHANGE_CURR_FY']\n",
    "\n",
    "\n",
    "#create a pipeline for numeric transformer\n",
    "numeric_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the categorical features\n",
    "categorical_features = ['SUBREGION','COUNTRY','CAREER_LEVEL','TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
    "                        'TENURE_LATEST_HIRE_DATE_BAND_LIN','JOB_TENURE_BAND_LIN','TIME_SINCE_LAST_SALARY_INCR_BAND',\n",
    "                        'GENDER','AGE_BAND','PRODUCT_LINE','PRODUCT_ASSOCIATION','RCODE_06','RCODE_07',\n",
    "                        'MANAGER_GENDER_DESC','NATIONALITY',\n",
    "                        'HIRE_EVENT_DESCRIPTION','MANAGER180_OVERALL_BAND_CURR_FY','ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
    "                        'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL']\n",
    "\n",
    "#create a pipeline for categorical tranformer\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown = 'ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the preprocessor step using columntransformer for both numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dummy classifier\n",
    "clf = DummyClassifier (strategy=\"uniform\", random_state = 48) #strategy = \"uniform\" means that the predictions are uniformly generated at randomly\n",
    "\n",
    "#define whole pipeline - chain the preprocessor step and classifier step\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('preprocessor', ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('scaler', MinMaxScaler())]),\n",
      "                                 ['NUM_SALARY_CHANGE_PREV_FY',\n",
      "                                  'NUM_SALARY_CHANGE_CURR_FY',\n",
      "                                  'NUM_ORG_CHANGE_PREV_FY',\n",
      "                                  'NUM_ORG_CHANGE_CURR_FY',\n",
      "                                  'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
      "                                  'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
      "                                  'NUM_MANAGER_CHANGE_PREV_FY',\n",
      "                                  'NUM_MANAGER_CHANGE_CURR_FY',\n",
      "                                  'NUM_JOB_CHANGE_PREV_FY',\n",
      "                                  'NUM_JOB_...\n",
      "                                  'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
      "                                  'TENURE_LATEST_HIRE_DATE_BAND_LIN',\n",
      "                                  'JOB_TENURE_BAND_LIN',\n",
      "                                  'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER',\n",
      "                                  'AGE_BAND', 'PRODUCT_LINE',\n",
      "                                  'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07',\n",
      "                                  'MANAGER_GENDER_DESC', 'NATIONALITY',\n",
      "                                  'HIRE_EVENT_DESCRIPTION',\n",
      "                                  'MANAGER180_OVERALL_BAND_CURR_FY',\n",
      "                                  'ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
      "                                  'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'])])), ('clf', DummyClassifier(random_state=48, strategy='uniform'))]\n"
     ]
    }
   ],
   "source": [
    "print(pipe.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the naive_model with X_train and y_train data\n",
    "\n",
    "naive_model = pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the naive_model to predict the test data\n",
    "\n",
    "y_pred_naive = naive_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.62       249\n",
      "           1       0.31      0.52      0.39       100\n",
      "\n",
      "    accuracy                           0.53       349\n",
      "   macro avg       0.52      0.53      0.51       349\n",
      "weighted avg       0.61      0.53      0.56       349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect the classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation data\n",
    "\n",
    "actual_pred_naive = naive_model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.46      0.58       268\n",
      "           1       0.23      0.53      0.32        81\n",
      "\n",
      "    accuracy                           0.48       349\n",
      "   macro avg       0.50      0.50      0.45       349\n",
      "weighted avg       0.64      0.48      0.52       349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect the classification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix for naive model\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, actual_pred_naive)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "ax.yaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.1.3 Logistic Regression**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define classifier\n",
    "clf = LogisticRegression(random_state = 48)\n",
    "\n",
    "#define the pipeline - chain the preprocessor step and the classifier step\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preprocessor',\n",
       "  ColumnTransformer(transformers=[('num',\n",
       "                                   Pipeline(steps=[('scaler', MinMaxScaler())]),\n",
       "                                   ['NUM_SALARY_CHANGE_PREV_FY',\n",
       "                                    'NUM_SALARY_CHANGE_CURR_FY',\n",
       "                                    'NUM_ORG_CHANGE_PREV_FY',\n",
       "                                    'NUM_ORG_CHANGE_CURR_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_PREV_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_CURR_FY',\n",
       "                                    'NUM_JOB_CHANGE_PREV_FY',\n",
       "                                    'NUM_JOB_...\n",
       "                                    'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
       "                                    'TENURE_LATEST_HIRE_DATE_BAND_LIN',\n",
       "                                    'JOB_TENURE_BAND_LIN',\n",
       "                                    'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER',\n",
       "                                    'AGE_BAND', 'PRODUCT_LINE',\n",
       "                                    'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07',\n",
       "                                    'MANAGER_GENDER_DESC', 'NATIONALITY',\n",
       "                                    'HIRE_EVENT_DESCRIPTION',\n",
       "                                    'MANAGER180_OVERALL_BAND_CURR_FY',\n",
       "                                    'ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
       "                                    'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'])])),\n",
       " ('clf', LogisticRegression())]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomised search cv with 5 fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=45, shuffle=True)\n",
    "#input the parameters for search space\n",
    "param_grid = {\n",
    "    'clf__C':[0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "    'clf__penalty':['l1','l2', 'elasticnet'],\n",
    "    'clf__solver': ['lbfgs','liblinear', 'sag', 'saga'],\n",
    "}\n",
    "\n",
    "#create the logistic regression classifier object\n",
    "#use scoring = 'f1_macro' as there are imbalanced classes\n",
    "rscv_lr = RandomizedSearchCV(estimator = pipe, param_distributions=param_grid, scoring='f1_macro', cv=skf, verbose=1, random_state=43)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the lr_model with X_train and y_train data\n",
    "\n",
    "lr_model = rscv_lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by randomised search are: {'clf__solver': 'liblinear', 'clf__penalty': 'l1', 'clf__C': 5.0}\n",
      "Best accuracy from randomised search is: 0.5789694395374253\n"
     ]
    }
   ],
   "source": [
    "#print best parameters and accuracy\n",
    "print ('Best parameters found by randomised search are:', lr_model.best_params_)\n",
    "\n",
    "print ('Best accuracy from randomised search is:', lr_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the lr_model to predict the testing data\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       249\n",
      "           1       0.37      0.17      0.23       100\n",
      "\n",
      "    accuracy                           0.68       349\n",
      "   macro avg       0.55      0.53      0.51       349\n",
      "weighted avg       0.62      0.68      0.64       349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect the classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation data\n",
    "\n",
    "actual_pred_lr = lr_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84       268\n",
      "           1       0.37      0.21      0.27        81\n",
      "\n",
      "    accuracy                           0.73       349\n",
      "   macro avg       0.58      0.55      0.55       349\n",
      "weighted avg       0.69      0.73      0.70       349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect the classification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#get mean of cross validation scores with training data\n",
    "skf = StratifiedKFold(n_splits=5, random_state = 45, shuffle=True)\n",
    "\n",
    "lr_cross_val = cross_val_score(pipe, X_train, y_train, cv = skf, scoring='f1_macro')\n",
    "lr_cross_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, actual_pred_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "ax.yaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<font size=\"3\">**Note: Logistic regression does not have feature importance, so we use coefficients to assess which variables are more impactful to the model**</font>\n",
    "\n",
    "https://stackoverflow.com/questions/66750706/sklearn-important-features-error-when-using-logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the categorical features first\n",
    "\n",
    "feat_names = (rscv_lr.best_estimator_.named_steps['preprocessor'].transformers_[1][1]['onehot'].get_feature_names(input_features=categorical_features))\n",
    "\n",
    "#combine categorical features and numerical features and name it 'feat_names'\n",
    "feat_names = np.concatenate([feat_names, numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the coefficients from lr_model\n",
    "lr_coefs = lr_model.best_estimator_.named_steps['clf'].coef_.squeeze().tolist()\n",
    "\n",
    "#assign the feature names to the coefficients\n",
    "labels_coef = list(zip(feat_names, lr_coefs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #put the coefficients into a dataframe and rename the columns\n",
    "lr_coef_data = pd.DataFrame(labels_coef)\n",
    "lr_coef_data.rename(columns = {0:'Features', 1:'Coefficients'}, inplace = True)\n",
    "\n",
    "lr_coef_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 10 features based on absolute coefficient values\n",
    "lr_feat_top10 = lr_coef_data.iloc[(-lr_coef_data['Coefficients'].abs()).argsort()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_feat_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot the top 10 features in descending order\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=\"Coefficients\", y=\"Features\", data=lr_feat_top10, color=\"#2C5967\")\n",
    "plt.title('Logistic Regression Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Step 4.2 Classification With Oversampling (SMOTENC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: SMOTENC is an oversampling technique commonly used to achieve balanced classes**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.2.1 Training-Testing-Validation Split**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert categorical columns to string\n",
    "\n",
    "cat_cols =  ['SUBREGION','COUNTRY','CAREER_LEVEL','TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
    "             'TENURE_LATEST_HIRE_DATE_BAND_LIN','JOB_TENURE_BAND_LIN','TIME_SINCE_LAST_SALARY_INCR_BAND',\n",
    "             'GENDER','AGE_BAND','PRODUCT_LINE','PRODUCT_ASSOCIATION','RCODE_06','RCODE_07',\n",
    "             'MANAGER_GENDER_DESC','NATIONALITY','HIRE_EVENT_DESCRIPTION',\n",
    "             'MANAGER180_OVERALL_BAND_CURR_FY','ATTAINMENT_REP_LEVEL_BAND_PREV_FY','MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL']\n",
    "\n",
    "\n",
    "df3[cat_cols] = df3[cat_cols].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the feature columns\n",
    "feature_cols = ['SUBREGION','COUNTRY','CAREER_LEVEL','TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
    "                'TENURE_LATEST_HIRE_DATE_BAND_LIN','JOB_TENURE_BAND_LIN','TIME_SINCE_LAST_SALARY_INCR_BAND',\n",
    "                'GENDER','AGE_BAND','PRODUCT_LINE','PRODUCT_ASSOCIATION','RCODE_06','RCODE_07',\n",
    "                'NATIONALITY','MANAGER_GENDER_DESC',\n",
    "                'HIRE_EVENT_DESCRIPTION','NUM_SALARY_CHANGE_PREV_FY','NUM_SALARY_CHANGE_CURR_FY',\n",
    "                'NUM_ORG_CHANGE_PREV_FY','NUM_ORG_CHANGE_CURR_FY','NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
    "                'NUM_CAREER_LEVEL_CHANGE_CURR_FY','NUM_MANAGER_CHANGE_PREV_FY','NUM_MANAGER_CHANGE_CURR_FY',\n",
    "                'NUM_JOB_CHANGE_PREV_FY','NUM_JOB_CHANGE_CURR_FY','NUM_LOCATION_CHANGE_PREV_FY','NUM_LOCATION_CHANGE_CURR_FY',\n",
    "                'MANAGER180_OVERALL_BAND_CURR_FY','ATTAINMENT_REP_LEVEL_BAND_PREV_FY','MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL']\n",
    "\n",
    "\n",
    "#define X and y\n",
    "\n",
    "X = df3[feature_cols]\n",
    "y = df3['ATTAINMENT_REP_LEVEL_BANDNUM_FY21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an dataframe using selected feature columns\n",
    "feature_df = df3[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBREGION                                  object\n",
       "COUNTRY                                    object\n",
       "CAREER_LEVEL                               object\n",
       "TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN    object\n",
       "TENURE_LATEST_HIRE_DATE_BAND_LIN           object\n",
       "JOB_TENURE_BAND_LIN                        object\n",
       "TIME_SINCE_LAST_SALARY_INCR_BAND           object\n",
       "GENDER                                     object\n",
       "AGE_BAND                                   object\n",
       "PRODUCT_LINE                               object\n",
       "PRODUCT_ASSOCIATION                        object\n",
       "RCODE_06                                   object\n",
       "RCODE_07                                   object\n",
       "NATIONALITY                                object\n",
       "MANAGER_GENDER_DESC                        object\n",
       "HIRE_EVENT_DESCRIPTION                     object\n",
       "NUM_SALARY_CHANGE_PREV_FY                   int64\n",
       "NUM_SALARY_CHANGE_CURR_FY                   int64\n",
       "NUM_ORG_CHANGE_PREV_FY                      int64\n",
       "NUM_ORG_CHANGE_CURR_FY                      int64\n",
       "NUM_CAREER_LEVEL_CHANGE_PREV_FY             int64\n",
       "NUM_CAREER_LEVEL_CHANGE_CURR_FY             int64\n",
       "NUM_MANAGER_CHANGE_PREV_FY                  int64\n",
       "NUM_MANAGER_CHANGE_CURR_FY                  int64\n",
       "NUM_JOB_CHANGE_PREV_FY                      int64\n",
       "NUM_JOB_CHANGE_CURR_FY                      int64\n",
       "NUM_LOCATION_CHANGE_PREV_FY                 int64\n",
       "NUM_LOCATION_CHANGE_CURR_FY                 int64\n",
       "MANAGER180_OVERALL_BAND_CURR_FY            object\n",
       "ATTAINMENT_REP_LEVEL_BAND_PREV_FY          object\n",
       "MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check datatypes\n",
    "feature_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use SMOTENC for oversampling\n",
    "smote_nc = SMOTENC(categorical_features=[X.dtypes==object], random_state=21)\n",
    "\n",
    "\n",
    "X_smote, y_smote = smote_nc.fit_resample(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the X_smote array back to dataframe\n",
    "X_smote = pd.DataFrame(data=X_smote, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2560, 31)\n",
      "(2560,)\n"
     ]
    }
   ],
   "source": [
    "print(X_smote.shape)\n",
    "print(y_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the data into 60% training data, 20% validation data and 20% testing data\n",
    "# validation dataset is used to ensure that the model would not have seen the whole dataset\n",
    "\n",
    "\n",
    "#split the dataset into training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=32)\n",
    "\n",
    "#split the training data again into training data and validation data. \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=32) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df2.iloc[1122])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.2.2 Logistic Regression with Oversampling**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define the numeric features\n",
    "numeric_features = ['NUM_SALARY_CHANGE_PREV_FY','NUM_SALARY_CHANGE_CURR_FY','NUM_ORG_CHANGE_PREV_FY','NUM_ORG_CHANGE_CURR_FY',\n",
    "                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY','NUM_CAREER_LEVEL_CHANGE_CURR_FY','NUM_MANAGER_CHANGE_PREV_FY',\n",
    "                    'NUM_MANAGER_CHANGE_CURR_FY','NUM_JOB_CHANGE_PREV_FY','NUM_JOB_CHANGE_CURR_FY',\n",
    "                    'NUM_LOCATION_CHANGE_PREV_FY','NUM_LOCATION_CHANGE_CURR_FY']\n",
    "\n",
    "#create a pipeline for numeric transformer\n",
    "numeric_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the categorical features\n",
    "categorical_features = ['SUBREGION','COUNTRY','CAREER_LEVEL','TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
    "                        'TENURE_LATEST_HIRE_DATE_BAND_LIN','JOB_TENURE_BAND_LIN','TIME_SINCE_LAST_SALARY_INCR_BAND',\n",
    "                        'GENDER','AGE_BAND','PRODUCT_LINE','PRODUCT_ASSOCIATION','RCODE_06','RCODE_07',\n",
    "                        'MANAGER_GENDER_DESC','NATIONALITY',\n",
    "                        'HIRE_EVENT_DESCRIPTION','MANAGER180_OVERALL_BAND_CURR_FY','ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
    "                        'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL']\n",
    "\n",
    "\n",
    "#create a pipeline for categorical transformer\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown = 'ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the preprocessor step using columntransformer for both numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define classifier\n",
    "clf = LogisticRegression(class_weight = 'balanced',random_state = 48)\n",
    "\n",
    "\n",
    "#define the whole pipeline - chain the column transformer and the classifier\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preprocessor',\n",
       "  ColumnTransformer(transformers=[('num',\n",
       "                                   Pipeline(steps=[('scaler', MinMaxScaler())]),\n",
       "                                   ['NUM_SALARY_CHANGE_PREV_FY',\n",
       "                                    'NUM_SALARY_CHANGE_CURR_FY',\n",
       "                                    'NUM_ORG_CHANGE_PREV_FY',\n",
       "                                    'NUM_ORG_CHANGE_CURR_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_PREV_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_CURR_FY',\n",
       "                                    'NUM_JOB_CHANGE_PREV_FY',\n",
       "                                    'NUM_JOB_...\n",
       "                                    'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
       "                                    'TENURE_LATEST_HIRE_DATE_BAND_LIN',\n",
       "                                    'JOB_TENURE_BAND_LIN',\n",
       "                                    'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER',\n",
       "                                    'AGE_BAND', 'PRODUCT_LINE',\n",
       "                                    'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07',\n",
       "                                    'MANAGER_GENDER_DESC', 'NATIONALITY',\n",
       "                                    'HIRE_EVENT_DESCRIPTION',\n",
       "                                    'MANAGER180_OVERALL_BAND_CURR_FY',\n",
       "                                    'ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
       "                                    'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'])])),\n",
       " ('clf', LogisticRegression(class_weight='balanced'))]"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomised search cv with 5 fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=45, shuffle=True)\n",
    "#input the parameters for search space\n",
    "param_grid = {\n",
    "    'clf__C':[0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "    'clf__penalty':['l1','l2', 'elasticnet'],\n",
    "    'clf__solver': ['lbfgs','liblinear', 'sag', 'saga'],\n",
    "}\n",
    "\n",
    "#create the logistic regression classifier object\n",
    "#use scoring = 'f1_macro' as there are imbalanced classes\n",
    "rscv_lr2 = RandomizedSearchCV(estimator = pipe, param_distributions=param_grid, scoring='f1_macro', cv=skf, verbose=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the lr_model2 with X_train and y_train data\n",
    "\n",
    "lr_model2 = rscv_lr2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "pkl_filename = 'fy21_lr_rscv_binarygoodnotgood_model.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(lr_model2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model if needed to save time\n",
    "with open ('fy21_lr_rscv_binarygoodnotgood_model.pkl','rb') as file:\n",
    "    lr_model2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by randomised search are: {'clf__solver': 'liblinear', 'clf__penalty': 'l1', 'clf__C': 0.5}\n",
      "Best accuracy from randomised search is: 0.7080765540924494\n"
     ]
    }
   ],
   "source": [
    "#print the best parameters and accuracy\n",
    "print ('Best parameters found by randomised search are:',lr_model2.best_params_)\n",
    "\n",
    "print ('Best accuracy from randomised search is:', lr_model2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#prediction on testing data\n",
    "y_pred_lr2 = lr_model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.74       260\n",
      "           1       0.73      0.74      0.73       252\n",
      "\n",
      "    accuracy                           0.73       512\n",
      "   macro avg       0.73      0.73      0.73       512\n",
      "weighted avg       0.73      0.73      0.73       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation data\n",
    "actual_pred_lr2 = lr_model2.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75       260\n",
      "           1       0.74      0.73      0.74       252\n",
      "\n",
      "    accuracy                           0.74       512\n",
      "   macro avg       0.74      0.74      0.74       512\n",
      "weighted avg       0.74      0.74      0.74       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classsification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predicted model\n",
    "pkl_filename = 'fy21_lr2predictionmodel.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(actual_pred_lr2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, actual_pred_lr2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "ax.yaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "\n",
    "\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "0.7080765540924494\n",
      "Wall time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#get the cross validation score for training data\n",
    "skf = StratifiedKFold(n_splits=5, random_state = 45, shuffle=True)\n",
    "\n",
    "lr_cross_val2 = cross_val_score(lr_model2, X_train, y_train, cv = skf, scoring='f1_macro')\n",
    "print(lr_cross_val2.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the categorical features first\n",
    "\n",
    "feat_names2 = (rscv_lr2.best_estimator_.named_steps['preprocessor'].transformers_[1][1]['onehot'].get_feature_names(input_features=categorical_features))\n",
    "\n",
    "#combine categorical features and numerical features and name it 'feat_names'\n",
    "feat_names2 = np.concatenate([feat_names2, numeric_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the coefficients from lr_model2\n",
    "lr_coefs2 = lr_model2.best_estimator_.named_steps['clf'].coef_.squeeze().tolist()\n",
    "\n",
    "#assign the feature names to the coefficients\n",
    "labels_coef2 = list(zip(feat_names2, lr_coefs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the coefficients into a dataframe and rename the columns\n",
    "lr_coef_data2 = pd.DataFrame(labels_coef2)\n",
    "lr_coef_data2.rename(columns = {0:'Features', 1:'Coefficients'}, inplace = True)\n",
    "\n",
    "lr_coef_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 10 features based on absolute coefficient values\n",
    "lr2_feat_top10 = lr_coef_data2.iloc[(-lr_coef_data2['Coefficients'].abs()).argsort()].head(10)\n",
    "lr2_feat_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 10 features in descending order\n",
    "plt.figure(figsize=(10, 10))\n",
    "features = sns.barplot(x=\"Coefficients\", y=\"Features\", data=lr2_feat_top10, color=\"#2C5967\")\n",
    "plt.title('Logistic Regression Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "figure = features.get_figure()    \n",
    "figure.savefig('LR_features.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.2.3.1 Random Forest with Oversampling**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define classifier\n",
    "clf = RandomForestClassifier(random_state = 48)\n",
    "\n",
    "#define the pipeline - chain the preprocessor step and the classifier\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preprocessor',\n",
       "  ColumnTransformer(transformers=[('num',\n",
       "                                   Pipeline(steps=[('scaler', MinMaxScaler())]),\n",
       "                                   ['NUM_SALARY_CHANGE_PREV_FY',\n",
       "                                    'NUM_SALARY_CHANGE_CURR_FY',\n",
       "                                    'NUM_ORG_CHANGE_PREV_FY',\n",
       "                                    'NUM_ORG_CHANGE_CURR_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_PREV_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_CURR_FY',\n",
       "                                    'NUM_JOB_CHANGE_PREV_FY',\n",
       "                                    'NUM_JOB_...\n",
       "                                    'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
       "                                    'TENURE_LATEST_HIRE_DATE_BAND_LIN',\n",
       "                                    'JOB_TENURE_BAND_LIN',\n",
       "                                    'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER',\n",
       "                                    'AGE_BAND', 'PRODUCT_LINE',\n",
       "                                    'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07',\n",
       "                                    'MANAGER_GENDER_DESC', 'NATIONALITY',\n",
       "                                    'HIRE_EVENT_DESCRIPTION',\n",
       "                                    'MANAGER180_OVERALL_BAND_CURR_FY',\n",
       "                                    'ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
       "                                    'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'])])),\n",
       " ('clf', RandomForestClassifier())]"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check pipe steps\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomised search with 5 fold cross validation\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=15, shuffle=True)\n",
    "#input the parameters for search space\n",
    "param_grid = {\n",
    "    'clf__n_estimators':[500, 1000, 5000],\n",
    "    'clf__max_features':['sqrt','log2'],\n",
    "    'clf__max_depth': [5, 10, 15, 20],\n",
    "    'clf__min_samples_split': [2,5,10,15],\n",
    "    'clf__min_samples_leaf': [2,5,10,15],\n",
    "    'clf__bootstrap': [True, False],\n",
    "    'clf__criterion': ['gini','entropy']\n",
    "}\n",
    "\n",
    "#create the random forest classifier object\n",
    "rscv_rf = RandomizedSearchCV(estimator = pipe, param_distributions=param_grid, scoring='f1_macro', cv = skf, verbose=1, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the rf_model with X_train and y_train data\n",
    "rf_model = rscv_rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "pkl_filename = 'fy21_rf_rscv_binarygoodnotgood_model.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(rf_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model if needed to save time\n",
    "with open ('fy21_rf_rscv_binarygoodnotgood_model.pkl','rb') as file:\n",
    "    rf_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by randomised search are: {'clf__n_estimators': 500, 'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': 'sqrt', 'clf__max_depth': 15, 'clf__criterion': 'gini', 'clf__bootstrap': True}\n",
      "Best accuracy from randomised search is: 0.7660838261272098\n"
     ]
    }
   ],
   "source": [
    "#print best parameters and accuracy\n",
    "print ('Best parameters found by randomised search are:', rf_model.best_params_)\n",
    "\n",
    "print ('Best accuracy from randomised search is:', rf_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on testing data\n",
    "pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       260\n",
      "           1       0.76      0.76      0.76       252\n",
      "\n",
      "    accuracy                           0.76       512\n",
      "   macro avg       0.76      0.76      0.76       512\n",
      "weighted avg       0.76      0.76      0.76       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 79.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation data\n",
    "\n",
    "actual_pred_rf = rf_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79       260\n",
      "           1       0.79      0.79      0.79       252\n",
      "\n",
      "    accuracy                           0.79       512\n",
      "   macro avg       0.79      0.79      0.79       512\n",
      "weighted avg       0.79      0.79      0.79       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predicted model\n",
    "pkl_filename = 'fy21_rfpredictionmodel.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(actual_pred_rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "conf_mat2 = confusion_matrix(y_val, actual_pred_rf)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat2, annot=True, fmt='d')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "ax.yaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "0.7675794852756506\n",
      "Wall time: 9min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#get the cross validation score on the X_train and y_train data\n",
    "skf = StratifiedKFold(n_splits=5, random_state = 45, shuffle=True)\n",
    "\n",
    "rf_cross_val = cross_val_score(rf_model, X_train, y_train, cv = skf, scoring='f1_macro')\n",
    "\n",
    "print(rf_cross_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Permutation based feature importance was used to select features that contributed most to algorithm accuracy, more info here:**</font>\n",
    "\n",
    "https://explained.ai/rf-importance/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Permutation based feature importance is used here\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#use the function on the validation data\n",
    "imps = permutation_importance(rf_model, X_val, y_val, n_repeats=30)\n",
    "importances = imps.importances_mean\n",
    "std = imps.importances_std\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "#print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_val.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place the features into a dataframe and rename the columns\n",
    "\n",
    "importances_rf = list(zip(importances, X.columns))\n",
    "\n",
    "rf_perm_impt = pd.DataFrame(importances_rf)\n",
    "\n",
    "rf_perm_impt.rename(columns = {0:'Impt Values', 1:'Features'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 10 features\n",
    "\n",
    "rf_perm_top10 = rf_perm_impt.sort_values(by=['Impt Values'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 10 features in descending order\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "features = sns.barplot(x=\"Impt Values\", y=\"Features\", data= rf_perm_top10.sort_values(by=\"Impt Values\", ascending=False), color=\"#2C5967\")\n",
    "plt.title('Random Forest Permutation Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "figure = features.get_figure()\n",
    "#save the figure for use\n",
    "figure.savefig('RF_features.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.2.3.2 Visualising a random forest for explanation purposes (Optional to run)**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define classifier\n",
    "clf = RandomForestClassifier(random_state = 48)\n",
    "\n",
    "#define the pipeline - chain the preprocessor step and the classifier\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomised search with 5 fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=45, shuffle=True)\n",
    "\n",
    "#input the parameters for search space, the depth of the tree is kept short to aid in explanation\n",
    "param_grid = {\n",
    "    'clf__n_estimators':[500, 1000, 5000],\n",
    "    'clf__max_features':['sqrt','log2'],\n",
    "    'clf__max_depth': [4],\n",
    "    'clf__min_samples_split': [2,5,10,15],\n",
    "    'clf__min_samples_leaf': [2,5,10,15],\n",
    "    'clf__bootstrap': [True, False],\n",
    "    'clf__criterion': ['gini','entropy']\n",
    "}\n",
    "\n",
    "#create the random forest classifier object\n",
    "rscv_rf_viz = RandomizedSearchCV(estimator = pipe, param_distributions=param_grid, scoring='f1_macro', cv=skf, verbose=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the rf_model_viz with X_train and y_train data\n",
    "\n",
    "rf_model_viz = rscv_rf_viz.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by randomised search are: {'clf__n_estimators': 5000, 'clf__min_samples_split': 10, 'clf__min_samples_leaf': 15, 'clf__max_features': 'sqrt', 'clf__max_depth': 4, 'clf__criterion': 'entropy', 'clf__bootstrap': False}\n",
      "Best accuracy from randomised search is: 0.7047016872511539\n"
     ]
    }
   ],
   "source": [
    "#print the best parameters and accuracy\n",
    "print ('Best parameters found by randomised search are:', rf_model_viz.best_params_)\n",
    "\n",
    "print ('Best accuracy from randomised search is:', rf_model_viz.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on testing data first\n",
    "y_pred_viz = rf_model_viz.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.70       260\n",
      "           1       0.69      0.75      0.72       252\n",
      "\n",
      "    accuracy                           0.71       512\n",
      "   macro avg       0.71      0.71      0.71       512\n",
      "weighted avg       0.71      0.71      0.71       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_viz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 610 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation data\n",
    "\n",
    "actual_pred_rf_viz = rf_model_viz.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71       260\n",
      "           1       0.70      0.71      0.71       252\n",
      "\n",
      "    accuracy                           0.71       512\n",
      "   macro avg       0.71      0.71      0.71       512\n",
      "weighted avg       0.71      0.71      0.71       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_rf_viz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"2103pt\" height=\"552pt\"\r\n",
       " viewBox=\"0.00 0.00 2102.50 552.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 548)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-548 2098.5,-548 2098.5,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<polygon fill=\"#fbfdfe\" stroke=\"black\" points=\"1135.5,-544 943.5,-544 943.5,-461 1135.5,-461 1135.5,-544\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1039.5\" y=\"-528.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">CAREER_LEVEL_IC4 &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1039.5\" y=\"-513.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 1.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1039.5\" y=\"-498.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 1536</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1039.5\" y=\"-483.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [760, 776]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1039.5\" y=\"-468.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 0</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<polygon fill=\"#f9e0ce\" stroke=\"black\" points=\"868.5,-425 738.5,-425 738.5,-342 868.5,-342 868.5,-425\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-409.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">GENDER_F &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-394.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.985</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-379.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 793</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-364.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [453, 340]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M957.621,-460.907C931.654,-448.034 903.105,-433.881 877.71,-421.29\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"879.021,-418.034 868.506,-416.728 875.911,-424.305 879.021,-418.034\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"876.493\" y=\"-436.718\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<polygon fill=\"#c4e2f7\" stroke=\"black\" points=\"1377,-425 1176,-425 1176,-342 1377,-342 1377,-425\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-409.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">PRODUCT_LINE_Apps &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-394.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.978</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-379.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 743</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-364.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [307, 436]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 0</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>0&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1121.73,-460.907C1142.22,-450.789 1164.32,-439.879 1185.23,-429.559\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1187,-432.586 1194.42,-425.021 1183.9,-426.309 1187,-432.586\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1186.39\" y=\"-444.997\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<polygon fill=\"#fcf2ea\" stroke=\"black\" points=\"545,-306 328,-306 328,-223 545,-223 545,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-290.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RCODE_07_Japan Systems &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-275.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.998</text>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-260.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 637</text>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [336, 301]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M738.311,-361.718C687.573,-345.542 615.8,-322.661 554.974,-303.27\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"555.901,-299.892 545.311,-300.189 553.775,-306.561 555.901,-299.892\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<polygon fill=\"#eeab7b\" stroke=\"black\" points=\"990,-306 617,-306 617,-223 990,-223 990,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-290.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">TIME_SINCE_LAST_SALARY_INCR_BAND_&lt;1 Yr &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-275.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.811</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-260.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 156</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [117, 39]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"803.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>1&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M803.5,-341.907C803.5,-333.649 803.5,-324.864 803.5,-316.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"807,-316.021 803.5,-306.021 800,-316.021 807,-316.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<polygon fill=\"#fdf6f0\" stroke=\"black\" points=\"365,-187 0,-187 0,-104 365,-104 365,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"182.5\" y=\"-171.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MANAGER180_OVERALL_BAND_CURR_FY_nan &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"182.5\" y=\"-156.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.999</text>\r\n",
       "<text text-anchor=\"middle\" x=\"182.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 622</text>\r\n",
       "<text text-anchor=\"middle\" x=\"182.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [323, 299]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"182.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M348.376,-222.907C326.212,-212.698 302.299,-201.683 279.718,-191.282\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.015,-188.026 270.468,-187.021 278.087,-194.384 281.015,-188.026\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<polygon fill=\"#e99457\" stroke=\"black\" points=\"489.5,-179.5 383.5,-179.5 383.5,-111.5 489.5,-111.5 489.5,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-164.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.567</text>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-149.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 15</text>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [13, 2]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"436.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M436.5,-222.907C436.5,-212.204 436.5,-200.615 436.5,-189.776\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"440,-189.667 436.5,-179.667 433,-189.667 440,-189.667\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<polygon fill=\"#e4f1fb\" stroke=\"black\" points=\"176.5,-68 58.5,-68 58.5,-0 176.5,-0 176.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"117.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.996</text>\r\n",
       "<text text-anchor=\"middle\" x=\"117.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 499</text>\r\n",
       "<text text-anchor=\"middle\" x=\"117.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [231, 268]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"117.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 0</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.296,-103.726C153.099,-94.9703 147.598,-85.7032 142.375,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"145.381,-75.1121 137.267,-68.2996 139.362,-78.6853 145.381,-75.1121\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<polygon fill=\"#eeab7c\" stroke=\"black\" points=\"300.5,-68 194.5,-68 194.5,-0 300.5,-0 300.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"247.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.814</text>\r\n",
       "<text text-anchor=\"middle\" x=\"247.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 123</text>\r\n",
       "<text text-anchor=\"middle\" x=\"247.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [92, 31]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"247.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.704,-103.726C211.901,-94.9703 217.402,-85.7032 222.625,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"225.638,-78.6853 227.733,-68.2996 219.619,-75.1121 225.638,-78.6853\"/>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<polygon fill=\"#f0b78e\" stroke=\"black\" points=\"789,-187 508,-187 508,-104 789,-104 789,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"648.5\" y=\"-171.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">JOB_TENURE_BAND_LIN_1 to 2 yrs &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"648.5\" y=\"-156.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.882</text>\r\n",
       "<text text-anchor=\"middle\" x=\"648.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 123</text>\r\n",
       "<text text-anchor=\"middle\" x=\"648.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [86, 37]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"648.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M749.724,-222.907C736.98,-213.288 723.289,-202.953 710.222,-193.09\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"712.272,-190.252 702.181,-187.021 708.054,-195.839 712.272,-190.252\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<polygon fill=\"#e78946\" stroke=\"black\" points=\"906,-179.5 807,-179.5 807,-111.5 906,-111.5 906,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"856.5\" y=\"-164.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.33</text>\r\n",
       "<text text-anchor=\"middle\" x=\"856.5\" y=\"-149.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 33</text>\r\n",
       "<text text-anchor=\"middle\" x=\"856.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [31, 2]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"856.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>7&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M821.888,-222.907C826.837,-211.983 832.203,-200.137 837.2,-189.107\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"840.537,-190.22 841.476,-179.667 834.161,-187.332 840.537,-190.22\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<polygon fill=\"#f3c5a4\" stroke=\"black\" points=\"632.5,-68 526.5,-68 526.5,-0 632.5,-0 632.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"579.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.935</text>\r\n",
       "<text text-anchor=\"middle\" x=\"579.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 77</text>\r\n",
       "<text text-anchor=\"middle\" x=\"579.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [50, 27]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"579.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M622.807,-103.726C617.289,-94.9703 611.45,-85.7032 605.906,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"608.776,-74.8941 600.483,-68.2996 602.853,-78.6259 608.776,-74.8941\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<polygon fill=\"#eca470\" stroke=\"black\" points=\"756.5,-68 650.5,-68 650.5,-0 756.5,-0 756.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"703.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.755</text>\r\n",
       "<text text-anchor=\"middle\" x=\"703.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 46</text>\r\n",
       "<text text-anchor=\"middle\" x=\"703.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [36, 10]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"703.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>8&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M668.98,-103.726C673.286,-95.1527 677.839,-86.0891 682.175,-77.4555\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"685.413,-78.8067 686.774,-68.2996 679.158,-75.6647 685.413,-78.8067\"/>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<polygon fill=\"#a6d3f3\" stroke=\"black\" points=\"1477.5,-306 1075.5,-306 1075.5,-223 1477.5,-223 1477.5,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-290.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MANAGER180_OVERALL_BAND_CURR_FY_Very High &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-275.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.939</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-260.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 602</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [214, 388]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1276.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 0</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;13 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1276.5,-341.907C1276.5,-333.649 1276.5,-324.864 1276.5,-316.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1280,-316.021 1276.5,-306.021 1273,-316.021 1280,-316.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 20 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>20</title>\r\n",
       "<polygon fill=\"#f2c29f\" stroke=\"black\" points=\"1981,-306 1588,-306 1588,-223 1981,-223 1981,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-290.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">TENURE_LATEST_HIRE_DATE_BAND_LIN_1 to 2 yrs &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-275.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.925</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-260.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 141</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [93, 48]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;20 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>12&#45;&gt;20</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1377.17,-359.315C1440.28,-344.778 1523.59,-325.593 1598.4,-308.361\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1599.47,-311.707 1608.43,-306.052 1597.9,-304.886 1599.47,-311.707\"/>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<polygon fill=\"#f2bf9b\" stroke=\"black\" points=\"1090.5,-187 924.5,-187 924.5,-104 1090.5,-104 1090.5,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1007.5\" y=\"-171.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">COUNTRY_China &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1007.5\" y=\"-156.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.915</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1007.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 115</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1007.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [77, 38]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1007.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;14 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>13&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1182.29,-222.923C1157.93,-212.394 1131.57,-200.975 1100.18,-187.291\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1101.3,-183.96 1090.73,-183.171 1098.5,-190.377 1101.3,-183.96\"/>\r\n",
       "</g>\r\n",
       "<!-- 17 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>17</title>\r\n",
       "<polygon fill=\"#87c3ef\" stroke=\"black\" points=\"1580.5,-187 1108.5,-187 1108.5,-104 1580.5,-104 1580.5,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1344.5\" y=\"-171.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN_10 to 20 yrs &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1344.5\" y=\"-156.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.857</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1344.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 487</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1344.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [137, 350]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1344.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 0</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;17 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>13&#45;&gt;17</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1300.09,-222.907C1305.16,-214.195 1310.56,-204.897 1315.79,-195.893\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1318.95,-197.425 1320.95,-187.021 1312.9,-193.908 1318.95,-197.425\"/>\r\n",
       "</g>\r\n",
       "<!-- 15 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>15</title>\r\n",
       "<polygon fill=\"#f4c8a9\" stroke=\"black\" points=\"974.5,-68 868.5,-68 868.5,-0 974.5,-0 974.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.944</text>\r\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 94</text>\r\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [60, 34]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;15 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>14&#45;&gt;15</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M975.477,-103.726C968.457,-94.7878 961.018,-85.3168 953.98,-76.3558\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"956.582,-74.0022 947.653,-68.2996 951.077,-78.3259 956.582,-74.0022\"/>\r\n",
       "</g>\r\n",
       "<!-- 16 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>16</title>\r\n",
       "<polygon fill=\"#eb9f68\" stroke=\"black\" points=\"1098.5,-68 992.5,-68 992.5,-0 1098.5,-0 1098.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1045.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.702</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1045.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 21</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1045.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [17, 4]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1045.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;16 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>14&#45;&gt;16</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1021.65,-103.726C1024.56,-95.3351 1027.64,-86.4745 1030.58,-78.0072\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1033.97,-78.8944 1033.94,-68.2996 1027.36,-76.5994 1033.97,-78.8944\"/>\r\n",
       "</g>\r\n",
       "<!-- 18 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>18</title>\r\n",
       "<polygon fill=\"#7fc0ee\" stroke=\"black\" points=\"1338.5,-68 1220.5,-68 1220.5,-0 1338.5,-0 1338.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1279.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.829</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1279.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 451</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1279.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [118, 333]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1279.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 0</text>\r\n",
       "</g>\r\n",
       "<!-- 17&#45;&gt;18 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>17&#45;&gt;18</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1320.3,-103.726C1315.1,-94.9703 1309.6,-85.7032 1304.38,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1307.38,-75.1121 1299.27,-68.2996 1301.36,-78.6853 1307.38,-75.1121\"/>\r\n",
       "</g>\r\n",
       "<!-- 19 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>19</title>\r\n",
       "<polygon fill=\"#fcf2ea\" stroke=\"black\" points=\"1462.5,-68 1356.5,-68 1356.5,-0 1462.5,-0 1462.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1409.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.998</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1409.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 36</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1409.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [19, 17]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1409.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 17&#45;&gt;19 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>17&#45;&gt;19</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1368.7,-103.726C1373.9,-94.9703 1379.4,-85.7032 1384.62,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1387.64,-78.6853 1389.73,-68.2996 1381.62,-75.1121 1387.64,-78.6853\"/>\r\n",
       "</g>\r\n",
       "<!-- 21 -->\r\n",
       "<g id=\"node22\" class=\"node\"><title>21</title>\r\n",
       "<polygon fill=\"#f5d0b5\" stroke=\"black\" points=\"1970,-187 1599,-187 1599,-104 1970,-104 1970,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-171.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HIRE_EVENT_DESCRIPTION_TRANSFER EVENT &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-156.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.961</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 117</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [72, 45]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1784.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 20&#45;&gt;21 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>20&#45;&gt;21</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1784.5,-222.907C1784.5,-214.649 1784.5,-205.864 1784.5,-197.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1788,-197.021 1784.5,-187.021 1781,-197.021 1788,-197.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 24 -->\r\n",
       "<g id=\"node25\" class=\"node\"><title>24</title>\r\n",
       "<polygon fill=\"#e99355\" stroke=\"black\" points=\"2094.5,-179.5 1988.5,-179.5 1988.5,-111.5 2094.5,-111.5 2094.5,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2041.5\" y=\"-164.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.544</text>\r\n",
       "<text text-anchor=\"middle\" x=\"2041.5\" y=\"-149.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 24</text>\r\n",
       "<text text-anchor=\"middle\" x=\"2041.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [21, 3]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"2041.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 20&#45;&gt;24 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>20&#45;&gt;24</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1899.4,-222.941C1926.16,-212.264 1954.21,-200.046 1979.5,-187 1981.03,-186.21 1982.57,-185.391 1984.12,-184.55\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1986.02,-187.495 1992.98,-179.511 1982.56,-181.41 1986.02,-187.495\"/>\r\n",
       "</g>\r\n",
       "<!-- 22 -->\r\n",
       "<g id=\"node23\" class=\"node\"><title>22</title>\r\n",
       "<polygon fill=\"#f8e0ce\" stroke=\"black\" points=\"1775.5,-68 1669.5,-68 1669.5,-0 1775.5,-0 1775.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.985</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 98</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [56, 42]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 21&#45;&gt;22 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>21&#45;&gt;22</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1761.41,-103.726C1756.51,-95.0615 1751.32,-85.8962 1746.38,-77.1802\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1749.33,-75.277 1741.35,-68.2996 1743.24,-78.726 1749.33,-75.277\"/>\r\n",
       "</g>\r\n",
       "<!-- 23 -->\r\n",
       "<g id=\"node24\" class=\"node\"><title>23</title>\r\n",
       "<polygon fill=\"#ea995e\" stroke=\"black\" points=\"1899.5,-68 1793.5,-68 1793.5,-0 1899.5,-0 1899.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1846.5\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">entropy = 0.629</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1846.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 19</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1846.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [16, 3]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1846.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 21&#45;&gt;23 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>21&#45;&gt;23</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1807.59,-103.726C1812.49,-95.0615 1817.68,-85.8962 1822.62,-77.1802\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1825.76,-78.726 1827.65,-68.2996 1819.67,-75.277 1825.76,-78.726\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1704694ff70>"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use graphviz package for visualisation\n",
    "import graphviz\n",
    "# DOT data\n",
    "dot_data = tree.export_graphviz(rscv_rf_viz.best_estimator_.named_steps['clf'].estimators_[2], out_file=None, \n",
    "                                feature_names=fn,  \n",
    "                                class_names=cn,\n",
    "                                filled=True)\n",
    "\n",
    "# Draw graph\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fy21_decision_tree_graphivz(not good vs good).png'"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#render and save the image\n",
    "graph.render(\"decision_tree_graphivz\")\n",
    "'fy21_decision_tree_graphivz(not good vs good).png'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Refer to the following link on how to interpret the decision tree output**</font>\n",
    "\n",
    "https://stackoverflow.com/questions/47503575/what-do-the-values-that-graphviz-renders-inside-each-node-of-a-decision-tree-m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.3 Light Gradient Boosting Machine with Oversampling**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define classifier\n",
    "clf = LGBMClassifier(objective = 'binary', metric = 'binary_logloss',random_state = 48)\n",
    "\n",
    "#define the pipeline - chain the preprocessor step and the classifier\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preprocessor',\n",
       "  ColumnTransformer(transformers=[('num',\n",
       "                                   Pipeline(steps=[('scaler', MinMaxScaler())]),\n",
       "                                   ['NUM_SALARY_CHANGE_PREV_FY',\n",
       "                                    'NUM_SALARY_CHANGE_CURR_FY',\n",
       "                                    'NUM_ORG_CHANGE_PREV_FY',\n",
       "                                    'NUM_ORG_CHANGE_CURR_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_PREV_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_CURR_FY',\n",
       "                                    'NUM_JOB_CHANGE_PREV_FY',\n",
       "                                    'NUM_JOB_...\n",
       "                                    'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
       "                                    'TENURE_LATEST_HIRE_DATE_BAND_LIN',\n",
       "                                    'JOB_TENURE_BAND_LIN',\n",
       "                                    'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER',\n",
       "                                    'AGE_BAND', 'PRODUCT_LINE',\n",
       "                                    'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07',\n",
       "                                    'MANAGER_GENDER_DESC', 'NATIONALITY',\n",
       "                                    'HIRE_EVENT_DESCRIPTION',\n",
       "                                    'MANAGER180_OVERALL_BAND_CURR_FY',\n",
       "                                    'ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
       "                                    'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'])])),\n",
       " ('clf', LGBMClassifier(metric='binary_logloss', objective='binary'))]"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check pipe steps\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomised search with 5 fold cross validation\n",
    "\n",
    "#input the parameters for search space\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=15, shuffle=True)\n",
    "\n",
    "param_grid = {\n",
    "    'clf__boosting_type':['gbdt','dart'],\n",
    "    'clf__n_estimators':[500, 1000],\n",
    "    'clf__learning_rate':[0.05, 0.1, 0.5],\n",
    "    'clf__max_depth': [10, 20, 30],\n",
    "    'clf__num_leaves': [1000, 2000],\n",
    "\n",
    "}\n",
    "\n",
    "#create the light gbm classifier object\n",
    "#f1 macro is used for imbalanced datasets\n",
    "rscv_lgbm = RandomizedSearchCV(estimator = pipe, param_distributions=param_grid, scoring='f1_macro',random_state=22, cv=skf, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the lgbm_model with X_train and y_train data\n",
    "\n",
    "lgbm_model = rscv_lgbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "pkl_filename = 'fy21_lgbm_rscv_binarygoodnotgood_model.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(lgbm_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model if needed to save time\n",
    "with open ('fy21_lgbm_rscv_binarygoodnotgood_model.pkl','rb') as file:\n",
    "    lgbm_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by randomised search are: {'clf__num_leaves': 1000, 'clf__n_estimators': 1000, 'clf__max_depth': 30, 'clf__learning_rate': 0.1, 'clf__boosting_type': 'gbdt'}\n",
      "Best accuracy from randomised search is: 0.7678807619272018\n"
     ]
    }
   ],
   "source": [
    "#print best parameters and accuracy\n",
    "print ('Best parameters found by randomised search are:', lgbm_model.best_params_)\n",
    "\n",
    "print ('Best accuracy from randomised search is:', lgbm_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test data first\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76       260\n",
      "           1       0.75      0.75      0.75       252\n",
      "\n",
      "    accuracy                           0.76       512\n",
      "   macro avg       0.76      0.76      0.76       512\n",
      "weighted avg       0.76      0.76      0.76       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation daata\n",
    "\n",
    "actual_pred_lgbm = lgbm_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79       260\n",
      "           1       0.78      0.79      0.78       252\n",
      "\n",
      "    accuracy                           0.79       512\n",
      "   macro avg       0.79      0.79      0.79       512\n",
      "weighted avg       0.79      0.79      0.79       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predicted model\n",
    "pkl_filename = 'fy21_lgbmpredictionmodel.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(actual_pred_lgbm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, actual_pred_lgbm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "ax.yaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "0.7863243137450202\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#get the cross validation score on the X_train and y_train data\n",
    "skf = StratifiedKFold(n_splits=5, random_state = 45, shuffle=True)\n",
    "\n",
    "lgbm_cross_val = cross_val_score(lgbm_model, X_train, y_train, cv = skf, scoring='f1_macro')\n",
    "print(lgbm_cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Permutation based feature importance is used here\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#use the permutation importance function on validation data\n",
    "imps = permutation_importance(lgbm_model, X_val, y_val, n_repeats=13)\n",
    "importances = imps.importances_mean\n",
    "std = imps.importances_std\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_val.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place the features into a dataframe and rename the columns\n",
    "\n",
    "importances_lgbm = list(zip(importances, X.columns))\n",
    "\n",
    "lgbm_perm_impt = pd.DataFrame(importances_lgbm)\n",
    "\n",
    "lgbm_perm_impt.rename(columns = {0:'Impt Values', 1:'Features'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 10 features\n",
    "\n",
    "lgbm_perm_top10 = lgbm_perm_impt.sort_values(by=['Impt Values'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 10 features in descending order\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "features = sns.barplot(x=\"Impt Values\", y=\"Features\", data= lgbm_perm_top10.sort_values(by=\"Impt Values\", ascending=False), color=\"#2C5967\")\n",
    "plt.title('Light GBM Permutation Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "figure = features.get_figure()\n",
    "#save the chart for use\n",
    "figure.savefig('LGBM_features.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.4 Support Vector Machine with Oversampling**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define classifier\n",
    "#probability is set to True so that the predict_proba function could be used later\n",
    "clf = SVC(probability=True,random_state = 48)\n",
    "\n",
    "#define the pipeline - chain the preprocessor step and the classifier\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preprocessor',\n",
       "  ColumnTransformer(transformers=[('num',\n",
       "                                   Pipeline(steps=[('scaler', MinMaxScaler())]),\n",
       "                                   ['NUM_SALARY_CHANGE_PREV_FY',\n",
       "                                    'NUM_SALARY_CHANGE_CURR_FY',\n",
       "                                    'NUM_ORG_CHANGE_PREV_FY',\n",
       "                                    'NUM_ORG_CHANGE_CURR_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_PREV_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_CURR_FY',\n",
       "                                    'NUM_JOB_CHANGE_PREV_FY',\n",
       "                                    'NUM_JOB_...\n",
       "                                    'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
       "                                    'TENURE_LATEST_HIRE_DATE_BAND_LIN',\n",
       "                                    'JOB_TENURE_BAND_LIN',\n",
       "                                    'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER',\n",
       "                                    'AGE_BAND', 'PRODUCT_LINE',\n",
       "                                    'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07',\n",
       "                                    'MANAGER_GENDER_DESC', 'NATIONALITY',\n",
       "                                    'HIRE_EVENT_DESCRIPTION',\n",
       "                                    'MANAGER180_OVERALL_BAND_CURR_FY',\n",
       "                                    'ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
       "                                    'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'])])),\n",
       " ('clf', SVC(probability=True))]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check pipe steps\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomised search with 5 fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=45, shuffle=True)\n",
    "#input the parameters for search space\n",
    "\n",
    "param_grid = {'clf__C': [0.1, 1, 10, 100, 1000], \n",
    "              'clf__gamma': [0.001, 0.01, 0.1, 1],\n",
    "              'clf__kernel': ['linear','poly','rbf','sigmoid'],\n",
    "              'clf__degree':[1,3]\n",
    "\n",
    "}\n",
    "\n",
    "#create the support vector machine classifier object\n",
    "rscv_svc = RandomizedSearchCV(estimator = pipe, param_distributions=param_grid, scoring='f1_macro',cv=skf, random_state=22, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Wall time: 59.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the svc_model with X_train and y_train data\n",
    "svc_model = rscv_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "pkl_filename = 'fy21_svc_rscv_binarygoodnotgood_model.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(svc_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model if needed to save time\n",
    "with open ('fy21_svc_rscv_binarygoodnotgood_model.pkl','rb') as file:\n",
    "    svc_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by randomised search are: {'clf__kernel': 'poly', 'clf__gamma': 1, 'clf__degree': 3, 'clf__C': 1000}\n",
      "Best accuracy from randomised search is: 0.7449589438417745\n"
     ]
    }
   ],
   "source": [
    "#print the best parameters and accuracy\n",
    "print ('Best parameters found by randomised search are:', svc_model.best_params_)\n",
    "\n",
    "print ('Best accuracy from randomised search is:', svc_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test data first\n",
    "y_pred_svc = svc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75       260\n",
      "           1       0.74      0.73      0.74       252\n",
      "\n",
      "    accuracy                           0.74       512\n",
      "   macro avg       0.74      0.74      0.74       512\n",
      "weighted avg       0.74      0.74      0.74       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation data\n",
    "\n",
    "actual_pred_svc = svc_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76       260\n",
      "           1       0.75      0.77      0.76       252\n",
      "\n",
      "    accuracy                           0.76       512\n",
      "   macro avg       0.76      0.76      0.76       512\n",
      "weighted avg       0.76      0.76      0.76       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predicted model\n",
    "pkl_filename = 'fy21_svcpredictionmodel.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(actual_pred_svc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, actual_pred_svc)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "ax.yaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "0.7449589438417745\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#get the cross validation score on the X_train and y_train data\n",
    "skf = StratifiedKFold(n_splits=5, random_state = 45, shuffle=True)\n",
    "\n",
    "svc_cross_val = cross_val_score(svc_model, X_train, y_train, cv = skf, scoring='f1_macro')\n",
    "print(svc_cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Permutation based feature importance is used here\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#use the permutation importance function on the validation data\n",
    "imps = permutation_importance(svc_model, X_val, y_val, n_repeats=30)\n",
    "importances = imps.importances_mean\n",
    "std = imps.importances_std\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_val.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place the features into a dataframe and rename the columns\n",
    "\n",
    "importances2 = list(zip(importances, X.columns))\n",
    "\n",
    "svc_perm_impt = pd.DataFrame(importances2)\n",
    "\n",
    "svc_perm_impt.rename(columns = {0:'Impt Values', 1:'Features'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 10 features\n",
    "\n",
    "svc_perm_top10 = svc_perm_impt.sort_values(by=['Impt Values'], ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 10 features in descending order\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "features = sns.barplot(x=\"Impt Values\", y=\"Features\", data=svc_perm_top10.sort_values(by=\"Impt Values\", ascending=False), color=\"#2C5967\")\n",
    "plt.title('Support Vector Classifier Permutation Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "figure = features.get_figure()\n",
    "#save the chart for use later\n",
    "figure.savefig('SVC_features.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**4.5 Shallow Neural Network with Oversampling**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Keras Classifier is used so that integration with sklearn functions such as pipeline and randomised search would be possible**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#import keras_tuner as kt\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_network(unit,learn_rate=0.001):\n",
    "    nn_model_best = keras.Sequential()\n",
    "    # For the input layer, the units would be subjected to search\n",
    "    nn_model_best.add(keras.layers.Dense(units=unit, activation='relu'))\n",
    "    \n",
    "    # Only one hidden layer is used, which means it is a shallow network\n",
    "    nn_model_best.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Learning rate would be subjected to search\n",
    "    opt = keras.optimizers.Adam(learning_rate= learn_rate)\n",
    "    \n",
    "    #binary_crossentropy is used for loss function as outcome is binary\n",
    "    lossfunc = keras.losses.binary_crossentropy\n",
    "    \n",
    "    nn_model_best.compile(loss=lossfunc,optimizer=opt,metrics=['accuracy'])\n",
    "    \n",
    "    return nn_model_best\n",
    "\n",
    "\n",
    "#define classifier\n",
    "clf = KerasClassifier(build_fn=create_network,verbose=1)\n",
    "\n",
    "#define the pipeline - chain the preprocessor step and the classifier\n",
    "pipe = Pipeline([('preprocessor',preprocessor), ('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preprocessor',\n",
       "  ColumnTransformer(transformers=[('num',\n",
       "                                   Pipeline(steps=[('scaler', MinMaxScaler())]),\n",
       "                                   ['NUM_SALARY_CHANGE_PREV_FY',\n",
       "                                    'NUM_SALARY_CHANGE_CURR_FY',\n",
       "                                    'NUM_ORG_CHANGE_PREV_FY',\n",
       "                                    'NUM_ORG_CHANGE_CURR_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_PREV_FY',\n",
       "                                    'NUM_CAREER_LEVEL_CHANGE_CURR_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_PREV_FY',\n",
       "                                    'NUM_MANAGER_CHANGE_CURR_FY',\n",
       "                                    'NUM_JOB_CHANGE_PREV_FY',\n",
       "                                    'NUM_JOB_...\n",
       "                                    'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN',\n",
       "                                    'TENURE_LATEST_HIRE_DATE_BAND_LIN',\n",
       "                                    'JOB_TENURE_BAND_LIN',\n",
       "                                    'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER',\n",
       "                                    'AGE_BAND', 'PRODUCT_LINE',\n",
       "                                    'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07',\n",
       "                                    'MANAGER_GENDER_DESC', 'NATIONALITY',\n",
       "                                    'HIRE_EVENT_DESCRIPTION',\n",
       "                                    'MANAGER180_OVERALL_BAND_CURR_FY',\n",
       "                                    'ATTAINMENT_REP_LEVEL_BAND_PREV_FY',\n",
       "                                    'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'])])),\n",
       " ('clf', <keras.wrappers.scikit_learn.KerasClassifier at 0x2cee0169790>)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomised search with 5 fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=45, shuffle=True)\n",
    "\n",
    "#input the parameters for search space\n",
    "param_grid = {\n",
    "    'clf__unit': [5, 10, 15, 20, 25, 30],\n",
    "    'clf__learn_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'clf__epochs': [5, 10, 15, 20, 25, 30],\n",
    "    'clf__batch_size': [5, 10, 50, 100]\n",
    "}\n",
    "\n",
    "\n",
    "#create the neural network classifier object\n",
    "rscv_nn = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, cv=skf, random_state=22, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Epoch 1/15\n",
      "123/123 [==============================] - 1s 775us/step - loss: 0.9028 - accuracy: 0.5391\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - 0s 801us/step - loss: 0.7072 - accuracy: 0.4919\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - 0s 770us/step - loss: 0.6964 - accuracy: 0.5130\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - 0s 907us/step - loss: 0.7141 - accuracy: 0.5318\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - 0s 875us/step - loss: 0.7047 - accuracy: 0.5122\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7071 - accuracy: 0.4910\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.4984\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6997 - accuracy: 0.4927\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - 0s 817us/step - loss: 0.6956 - accuracy: 0.5171\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - 0s 883us/step - loss: 0.7005 - accuracy: 0.5057\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - 0s 809us/step - loss: 0.6991 - accuracy: 0.5228\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - 0s 948us/step - loss: 0.6979 - accuracy: 0.4870\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - 0s 818us/step - loss: 0.7015 - accuracy: 0.4870\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - 0s 858us/step - loss: 0.7116 - accuracy: 0.5024\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - 0s 956us/step - loss: 0.6978 - accuracy: 0.5098\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.6935 - accuracy: 0.5065\n",
      "Epoch 1/15\n",
      "123/123 [==============================] - 1s 1ms/step - loss: 0.8485 - accuracy: 0.4923\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7289 - accuracy: 0.4752\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7047 - accuracy: 0.5069\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7081 - accuracy: 0.5297\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7063 - accuracy: 0.4858\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7056 - accuracy: 0.4858\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7022 - accuracy: 0.4793\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7011 - accuracy: 0.4882\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7056 - accuracy: 0.5045\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7004 - accuracy: 0.5028\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6995 - accuracy: 0.4898\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7040 - accuracy: 0.4890\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7036 - accuracy: 0.4801\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7004 - accuracy: 0.4915\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.7168 - accuracy: 0.4988\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 0.6972 - accuracy: 0.4951\n",
      "Epoch 1/15\n",
      "123/123 [==============================] - 1s 1ms/step - loss: 0.8972 - accuracy: 0.4972\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7283 - accuracy: 0.5159\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6721 - accuracy: 0.5517\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6586 - accuracy: 0.5875\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6778 - accuracy: 0.5199\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6465 - accuracy: 0.5671\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6673 - accuracy: 0.5338\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6743 - accuracy: 0.5476\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.5443\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7014 - accuracy: 0.5159\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.6949 - accuracy: 0.4980\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6761 - accuracy: 0.5362\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6518 - accuracy: 0.5517\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7162 - accuracy: 0.5264\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6943 - accuracy: 0.5248\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.8314 - accuracy: 0.4951\n",
      "Epoch 1/15\n",
      "123/123 [==============================] - 1s 1ms/step - loss: 1.0319 - accuracy: 0.5207\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7238 - accuracy: 0.5012\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7024 - accuracy: 0.4955\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7018 - accuracy: 0.5004\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7046 - accuracy: 0.4972\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.5037\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.7086 - accuracy: 0.5110\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7143 - accuracy: 0.4874\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.7024 - accuracy: 0.5191\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.6992 - accuracy: 0.5110\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.6994 - accuracy: 0.5094\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.7019 - accuracy: 0.4931\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7038 - accuracy: 0.4646\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6963 - accuracy: 0.5191\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7099 - accuracy: 0.5037\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5049\n",
      "Epoch 1/15\n",
      "123/123 [==============================] - 1s 3ms/step - loss: 0.8831 - accuracy: 0.5362\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7355 - accuracy: 0.5020\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7331 - accuracy: 0.5134\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7041 - accuracy: 0.5134\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7372 - accuracy: 0.4996\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7009 - accuracy: 0.5094\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.5061\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.4890\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7018 - accuracy: 0.4784\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7037 - accuracy: 0.5004\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7039 - accuracy: 0.4931: 0s - loss: 0.7047 - accuracy: 0.49\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7069 - accuracy: 0.4988\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.7065 - accuracy: 0.4760\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.7039 - accuracy: 0.4736\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7056 - accuracy: 0.5134\n",
      "31/31 [==============================] - 0s 990us/step - loss: 0.6945 - accuracy: 0.5114\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 1ms/step - loss: 0.7677 - accuracy: 0.5285\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7212 - accuracy: 0.4845\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7128 - accuracy: 0.4756\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7063 - accuracy: 0.4943\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7062 - accuracy: 0.4731\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7047 - accuracy: 0.4959\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7136 - accuracy: 0.5049\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7011 - accuracy: 0.5073\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7033 - accuracy: 0.4951\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7067 - accuracy: 0.4772\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7044 - accuracy: 0.5041\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7056 - accuracy: 0.4870\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7014 - accuracy: 0.5228\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.5147\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7041 - accuracy: 0.5065\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7048 - accuracy: 0.5179\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7039 - accuracy: 0.4910\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7000 - accuracy: 0.4910\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6964 - accuracy: 0.5326\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7029 - accuracy: 0.5098\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7073 - accuracy: 0.4870\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7078 - accuracy: 0.4935\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7078 - accuracy: 0.4788\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7093 - accuracy: 0.4805\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7019 - accuracy: 0.5033\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7080 - accuracy: 0.4870\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7087 - accuracy: 0.4674\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7052 - accuracy: 0.4731\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7051 - accuracy: 0.4813\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7032 - accuracy: 0.4902\n",
      "62/62 [==============================] - 0s 732us/step - loss: 0.7410 - accuracy: 0.4935\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 768us/step - loss: 0.8097 - accuracy: 0.5224\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 819us/step - loss: 0.7228 - accuracy: 0.5216\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 893us/step - loss: 0.7059 - accuracy: 0.5004\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 976us/step - loss: 0.7074 - accuracy: 0.4858\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 968us/step - loss: 0.7015 - accuracy: 0.5151\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 929us/step - loss: 0.6985 - accuracy: 0.5004\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 875us/step - loss: 0.7059 - accuracy: 0.4736\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 972us/step - loss: 0.7046 - accuracy: 0.4972\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 945us/step - loss: 0.7077 - accuracy: 0.4947\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 926us/step - loss: 0.7016 - accuracy: 0.5151\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 954us/step - loss: 0.7020 - accuracy: 0.5028\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 940us/step - loss: 0.7071 - accuracy: 0.5077\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 908us/step - loss: 0.7028 - accuracy: 0.4947\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 982us/step - loss: 0.7050 - accuracy: 0.4687\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 950us/step - loss: 0.7057 - accuracy: 0.4874\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6983 - accuracy: 0.5061\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7081 - accuracy: 0.4573\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 950us/step - loss: 0.7047 - accuracy: 0.4866\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 955us/step - loss: 0.7017 - accuracy: 0.4931\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 945us/step - loss: 0.7050 - accuracy: 0.4955\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 998us/step - loss: 0.7021 - accuracy: 0.4955\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 996us/step - loss: 0.7034 - accuracy: 0.5151\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7055 - accuracy: 0.4898\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7067 - accuracy: 0.4980\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7042 - accuracy: 0.4947\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 975us/step - loss: 0.7030 - accuracy: 0.4980\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 956us/step - loss: 0.7007 - accuracy: 0.4858\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 927us/step - loss: 0.7037 - accuracy: 0.5134\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 911us/step - loss: 0.7090 - accuracy: 0.4817\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 944us/step - loss: 0.7094 - accuracy: 0.4972\n",
      "62/62 [==============================] - 0s 749us/step - loss: 0.6945 - accuracy: 0.5049\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 888us/step - loss: 0.7825 - accuracy: 0.5183\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7087 - accuracy: 0.4923\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7109 - accuracy: 0.4890\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7000 - accuracy: 0.5264\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 921us/step - loss: 0.7001 - accuracy: 0.5077\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 987us/step - loss: 0.7064 - accuracy: 0.4801\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 974us/step - loss: 0.7032 - accuracy: 0.4963\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.5224\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7040 - accuracy: 0.4980\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7035 - accuracy: 0.5045\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7003 - accuracy: 0.4963\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6966 - accuracy: 0.5378\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4776\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7034 - accuracy: 0.4898\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7004 - accuracy: 0.5037\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7069 - accuracy: 0.4947\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7047 - accuracy: 0.4833\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7003 - accuracy: 0.5061\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7005 - accuracy: 0.4996\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.5134\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7031 - accuracy: 0.5207\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.5061\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7046 - accuracy: 0.5020\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7076 - accuracy: 0.5069\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.5118\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7059 - accuracy: 0.4931\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.5118\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7006 - accuracy: 0.5313\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7034 - accuracy: 0.5053\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 993us/step - loss: 0.7041 - accuracy: 0.4963\n",
      "62/62 [==============================] - 0s 773us/step - loss: 0.7108 - accuracy: 0.5049\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 803us/step - loss: 0.7346 - accuracy: 0.5330\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 903us/step - loss: 0.7047 - accuracy: 0.4809\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 830us/step - loss: 0.7130 - accuracy: 0.5183\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 867us/step - loss: 0.7106 - accuracy: 0.4833\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 988us/step - loss: 0.7023 - accuracy: 0.4858\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 808us/step - loss: 0.7013 - accuracy: 0.4972\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 985us/step - loss: 0.6977 - accuracy: 0.5240\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 948us/step - loss: 0.7011 - accuracy: 0.5028\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 905us/step - loss: 0.6945 - accuracy: 0.5321\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 916us/step - loss: 0.7044 - accuracy: 0.4849\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 842us/step - loss: 0.7006 - accuracy: 0.4988\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 930us/step - loss: 0.7064 - accuracy: 0.4988\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 917us/step - loss: 0.6992 - accuracy: 0.5110\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 815us/step - loss: 0.7042 - accuracy: 0.4996\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 946us/step - loss: 0.6993 - accuracy: 0.4988\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 926us/step - loss: 0.7029 - accuracy: 0.4898\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 912us/step - loss: 0.7015 - accuracy: 0.5028\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 918us/step - loss: 0.7063 - accuracy: 0.4915\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 932us/step - loss: 0.7049 - accuracy: 0.4849\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 821us/step - loss: 0.7032 - accuracy: 0.4963\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 900us/step - loss: 0.7011 - accuracy: 0.5077\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 869us/step - loss: 0.7032 - accuracy: 0.4605\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 900us/step - loss: 0.7079 - accuracy: 0.4972\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 880us/step - loss: 0.7000 - accuracy: 0.5045\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 895us/step - loss: 0.7048 - accuracy: 0.5037\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 991us/step - loss: 0.7013 - accuracy: 0.4988\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 965us/step - loss: 0.7018 - accuracy: 0.5061\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 887us/step - loss: 0.7092 - accuracy: 0.5053\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 945us/step - loss: 0.7028 - accuracy: 0.4858\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 933us/step - loss: 0.7038 - accuracy: 0.5069\n",
      "62/62 [==============================] - 0s 698us/step - loss: 0.6903 - accuracy: 0.5081\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 741us/step - loss: 0.7650 - accuracy: 0.4760\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 905us/step - loss: 0.7011 - accuracy: 0.4947\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 881us/step - loss: 0.7124 - accuracy: 0.4980\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 910us/step - loss: 0.7339 - accuracy: 0.5199\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 917us/step - loss: 0.7046 - accuracy: 0.5183\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 887us/step - loss: 0.7034 - accuracy: 0.4736\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 905us/step - loss: 0.7087 - accuracy: 0.4849\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 906us/step - loss: 0.7065 - accuracy: 0.5028\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 957us/step - loss: 0.7017 - accuracy: 0.4866\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 858us/step - loss: 0.7027 - accuracy: 0.4866\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 915us/step - loss: 0.7010 - accuracy: 0.5110\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7025 - accuracy: 0.5028\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6996 - accuracy: 0.4898\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 962us/step - loss: 0.7075 - accuracy: 0.5151\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 960us/step - loss: 0.7039 - accuracy: 0.5004\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 942us/step - loss: 0.7106 - accuracy: 0.4849\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 918us/step - loss: 0.7076 - accuracy: 0.5085\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 976us/step - loss: 0.7031 - accuracy: 0.4988\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 919us/step - loss: 0.7100 - accuracy: 0.4963\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 900us/step - loss: 0.7052 - accuracy: 0.48820s - loss: 0.7110 - accuracy\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 872us/step - loss: 0.7032 - accuracy: 0.5004\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 939us/step - loss: 0.7009 - accuracy: 0.5159\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 930us/step - loss: 0.7019 - accuracy: 0.4923\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 951us/step - loss: 0.7019 - accuracy: 0.4858\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 939us/step - loss: 0.7077 - accuracy: 0.5012\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 921us/step - loss: 0.7094 - accuracy: 0.4858\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 978us/step - loss: 0.7037 - accuracy: 0.4947\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 863us/step - loss: 0.7100 - accuracy: 0.4882\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 885us/step - loss: 0.6987 - accuracy: 0.5248\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 926us/step - loss: 0.7029 - accuracy: 0.5151\n",
      "62/62 [==============================] - 0s 644us/step - loss: 0.6954 - accuracy: 0.4951\n",
      "Epoch 1/20\n",
      "123/123 [==============================] - 1s 790us/step - loss: 0.6568 - accuracy: 0.6295\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 0s 795us/step - loss: 0.5912 - accuracy: 0.6678\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 0s 887us/step - loss: 0.6003 - accuracy: 0.6539\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5463 - accuracy: 0.7280\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 0s 966us/step - loss: 0.5506 - accuracy: 0.7020\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7451\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 0s 997us/step - loss: 0.5215 - accuracy: 0.7150\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 0s 972us/step - loss: 0.5173 - accuracy: 0.7386\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 0s 951us/step - loss: 0.5074 - accuracy: 0.7239\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 0s 957us/step - loss: 0.5181 - accuracy: 0.7337\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 0s 948us/step - loss: 0.5035 - accuracy: 0.7484\n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 0s 976us/step - loss: 0.4950 - accuracy: 0.7581\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 0s 877us/step - loss: 0.4797 - accuracy: 0.7622\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 0s 977us/step - loss: 0.4738 - accuracy: 0.7451\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 0s 947us/step - loss: 0.4801 - accuracy: 0.7671\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 0s 866us/step - loss: 0.5125 - accuracy: 0.7402\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 0s 965us/step - loss: 0.5024 - accuracy: 0.7410\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 0s 953us/step - loss: 0.4858 - accuracy: 0.7451\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 0s 942us/step - loss: 0.4613 - accuracy: 0.7500\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4849 - accuracy: 0.7321\n",
      "31/31 [==============================] - 0s 774us/step - loss: 1.2399 - accuracy: 0.6818\n",
      "Epoch 1/20\n",
      "123/123 [==============================] - 1s 928us/step - loss: 0.6763 - accuracy: 0.6159\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6855 - accuracy: 0.6127\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 0s 984us/step - loss: 0.6523 - accuracy: 0.6444\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 0s 981us/step - loss: 0.6298 - accuracy: 0.6705\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 0s 970us/step - loss: 0.5994 - accuracy: 0.6819\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5982 - accuracy: 0.6884\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6021 - accuracy: 0.6843\n",
      "Epoch 8/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5939 - accuracy: 0.6892\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5845 - accuracy: 0.6957\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.7079\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.6867\n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5766 - accuracy: 0.7046\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.6973\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5973 - accuracy: 0.6949\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.6998\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5872 - accuracy: 0.6851\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.6981\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5812 - accuracy: 0.6786\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5671 - accuracy: 0.7022\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.7006\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 0.7423 - accuracy: 0.6612\n",
      "Epoch 1/20\n",
      "123/123 [==============================] - 1s 1ms/step - loss: 0.6378 - accuracy: 0.6631\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6078 - accuracy: 0.7030\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5716 - accuracy: 0.6892\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5406 - accuracy: 0.6998\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5300 - accuracy: 0.7307\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5188 - accuracy: 0.7136\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.7177\n",
      "Epoch 8/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7225\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5013 - accuracy: 0.7347\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.7518\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4983 - accuracy: 0.7372\n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4783 - accuracy: 0.7624\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.7445\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5754 - accuracy: 0.7006\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7242\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.7250\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4731 - accuracy: 0.7461\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.7315\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.7413\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.7543: 0s - loss: 0.5053 - accuracy: \n",
      "31/31 [==============================] - 0s 1ms/step - loss: 1.1182 - accuracy: 0.7036\n",
      "Epoch 1/20\n",
      "123/123 [==============================] - 1s 892us/step - loss: 0.6549 - accuracy: 0.6363\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5976 - accuracy: 0.6989\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5649 - accuracy: 0.7201\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5615 - accuracy: 0.7437\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5725 - accuracy: 0.7193\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5553 - accuracy: 0.7526\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5522 - accuracy: 0.7364\n",
      "Epoch 8/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.7469\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5361 - accuracy: 0.7567\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5385 - accuracy: 0.7404\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.7592\n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5476 - accuracy: 0.7347\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.7624\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5470 - accuracy: 0.7518\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5375 - accuracy: 0.7510\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7600\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5263 - accuracy: 0.7616\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5522 - accuracy: 0.7339\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7543\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5215 - accuracy: 0.7592\n",
      "31/31 [==============================] - 0s 997us/step - loss: 0.6446 - accuracy: 0.7231\n",
      "Epoch 1/20\n",
      "123/123 [==============================] - 1s 818us/step - loss: 0.7040 - accuracy: 0.5810\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 0s 991us/step - loss: 0.6640 - accuracy: 0.6200\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 0s 1000us/step - loss: 0.6670 - accuracy: 0.6363\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6119 - accuracy: 0.6753\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5945 - accuracy: 0.6908\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6043 - accuracy: 0.6908\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5922 - accuracy: 0.6949\n",
      "Epoch 8/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5793 - accuracy: 0.7103\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5786 - accuracy: 0.6965\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5829 - accuracy: 0.7063\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5758 - accuracy: 0.7136\n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.7152\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5826 - accuracy: 0.6924\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5781 - accuracy: 0.7087\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.7022\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7160\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7193\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5632 - accuracy: 0.7201\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6110 - accuracy: 0.6615\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.7160\n",
      "31/31 [==============================] - 0s 765us/step - loss: 0.6435 - accuracy: 0.6645\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 1ms/step - loss: 0.7162 - accuracy: 0.5855\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6845 - accuracy: 0.5546\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6989 - accuracy: 0.5489\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7012 - accuracy: 0.5277\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6892 - accuracy: 0.4878\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5244\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7017 - accuracy: 0.5212\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.4967\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6895 - accuracy: 0.5138\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6834 - accuracy: 0.5138\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6815 - accuracy: 0.5114\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6846 - accuracy: 0.5309\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5171\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6891 - accuracy: 0.5122\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6882 - accuracy: 0.5138\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6881 - accuracy: 0.5057\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6869 - accuracy: 0.5122\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.4992\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6825 - accuracy: 0.5204\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.5326\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6858 - accuracy: 0.5016\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6827 - accuracy: 0.5236\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6949 - accuracy: 0.5122\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.5130\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.4910\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6947 - accuracy: 0.5090\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6893 - accuracy: 0.5155\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.5228\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6974 - accuracy: 0.5138\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6887 - accuracy: 0.5318\n",
      "31/31 [==============================] - 0s 799us/step - loss: 1.1613 - accuracy: 0.5097\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 948us/step - loss: 0.8564 - accuracy: 0.5256\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7143 - accuracy: 0.5631\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7112 - accuracy: 0.5980\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6972 - accuracy: 0.5094\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.5297\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6866 - accuracy: 0.4988\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.4890\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6814 - accuracy: 0.5492\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7075 - accuracy: 0.4931\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7106 - accuracy: 0.4727\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7023 - accuracy: 0.4736\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7084 - accuracy: 0.4972\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.4793\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7091 - accuracy: 0.4931\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7212 - accuracy: 0.4915\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6986 - accuracy: 0.5224\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7025 - accuracy: 0.5037\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6952 - accuracy: 0.5321\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4825\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7038 - accuracy: 0.4809\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7059 - accuracy: 0.4833\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7018 - accuracy: 0.5028\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7061 - accuracy: 0.5199\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7009 - accuracy: 0.4947\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7037 - accuracy: 0.4963\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7092 - accuracy: 0.5037\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7017 - accuracy: 0.5053\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6963 - accuracy: 0.5199\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7059 - accuracy: 0.5045\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7070 - accuracy: 0.4955\n",
      "31/31 [==============================] - 0s 798us/step - loss: 0.8394 - accuracy: 0.4658\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 965us/step - loss: 0.8452 - accuracy: 0.5240\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 999us/step - loss: 0.7099 - accuracy: 0.5256\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7066 - accuracy: 0.5248\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7087 - accuracy: 0.5256\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7282 - accuracy: 0.4931\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7156 - accuracy: 0.5167\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7017 - accuracy: 0.4719\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7032 - accuracy: 0.4744\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.5264\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7088 - accuracy: 0.4996\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6995 - accuracy: 0.5142\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7065 - accuracy: 0.4825\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7019 - accuracy: 0.5037\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7004 - accuracy: 0.5102\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7043 - accuracy: 0.5077\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7097 - accuracy: 0.4874\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7094 - accuracy: 0.5183\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7067 - accuracy: 0.5037\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7017 - accuracy: 0.4931\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4817\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7042 - accuracy: 0.4849\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7020 - accuracy: 0.5297\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7080 - accuracy: 0.5004\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7117 - accuracy: 0.4874\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7034 - accuracy: 0.4882\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7143 - accuracy: 0.4898\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.5085\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7033 - accuracy: 0.4841\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6975 - accuracy: 0.5191\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7034 - accuracy: 0.4939\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.7195 - accuracy: 0.5049\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 938us/step - loss: 0.8964 - accuracy: 0.4955\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7025 - accuracy: 0.4915\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.4931\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7031 - accuracy: 0.5037\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 989us/step - loss: 0.6990 - accuracy: 0.5061\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7105 - accuracy: 0.4727\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7014 - accuracy: 0.4939\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7092 - accuracy: 0.4890\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7041 - accuracy: 0.5110\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7016 - accuracy: 0.5053\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6997 - accuracy: 0.5077\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7009 - accuracy: 0.5069\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7070 - accuracy: 0.4825\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7095 - accuracy: 0.5126\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6978 - accuracy: 0.5061\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 969us/step - loss: 0.7016 - accuracy: 0.4906\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7064 - accuracy: 0.4906\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7047 - accuracy: 0.4996\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6982 - accuracy: 0.5110\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7022 - accuracy: 0.5207\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7078 - accuracy: 0.4988\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7027 - accuracy: 0.5004\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7024 - accuracy: 0.5069\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7032 - accuracy: 0.4931\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7070 - accuracy: 0.5037\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7122 - accuracy: 0.5028\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.5126\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7044 - accuracy: 0.4898\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7088 - accuracy: 0.4727\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6955 - accuracy: 0.5224\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.6943 - accuracy: 0.4951\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 943us/step - loss: 0.8743 - accuracy: 0.5679\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 913us/step - loss: 0.7792 - accuracy: 0.5305\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7044 - accuracy: 0.4898\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6999 - accuracy: 0.5151\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7094 - accuracy: 0.4882\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.4955\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7048 - accuracy: 0.5004\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4801\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6984 - accuracy: 0.5012\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7028 - accuracy: 0.5191\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7069 - accuracy: 0.4793\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7009 - accuracy: 0.4980\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7013 - accuracy: 0.4833\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7077 - accuracy: 0.4996\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7100 - accuracy: 0.4866\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7171 - accuracy: 0.4679\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6992 - accuracy: 0.4874\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7153 - accuracy: 0.5094\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7041 - accuracy: 0.4882\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6987 - accuracy: 0.5012\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7035 - accuracy: 0.5077\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7020 - accuracy: 0.4980\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7013 - accuracy: 0.4972\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.5118\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6992 - accuracy: 0.4882\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7095 - accuracy: 0.4801\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6995 - accuracy: 0.5175\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.5118\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7060 - accuracy: 0.4841\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7059 - accuracy: 0.4589\n",
      "31/31 [==============================] - 0s 798us/step - loss: 0.6935 - accuracy: 0.4951\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 876us/step - loss: 0.6887 - accuracy: 0.6083\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 944us/step - loss: 0.6612 - accuracy: 0.5969\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 995us/step - loss: 0.6707 - accuracy: 0.6238\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6560 - accuracy: 0.6181\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6502 - accuracy: 0.6205\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6381 - accuracy: 0.6555\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6045 - accuracy: 0.6938\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6067 - accuracy: 0.6914\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6438 - accuracy: 0.6254\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5957 - accuracy: 0.6612\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.6832\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5726 - accuracy: 0.6531\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6702\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5676 - accuracy: 0.6466\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5987 - accuracy: 0.6132\n",
      "62/62 [==============================] - 0s 853us/step - loss: 0.7907 - accuracy: 0.6071\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 1ms/step - loss: 0.7061 - accuracy: 0.5216\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6772 - accuracy: 0.5631\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6964 - accuracy: 0.5566\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6975 - accuracy: 0.5362\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6818 - accuracy: 0.5313\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6970 - accuracy: 0.5443\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7055 - accuracy: 0.5151\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6813 - accuracy: 0.5476\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6711 - accuracy: 0.5370\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6706 - accuracy: 0.5297\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5191\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5142\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6785 - accuracy: 0.5134\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5460\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6891 - accuracy: 0.5037\n",
      "62/62 [==============================] - 0s 965us/step - loss: 0.6917 - accuracy: 0.5147\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 941us/step - loss: 0.7168 - accuracy: 0.5476\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 998us/step - loss: 0.6949 - accuracy: 0.5085\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5346\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7161 - accuracy: 0.5151\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7013 - accuracy: 0.5126\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6681 - accuracy: 0.5590\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6695 - accuracy: 0.5736\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6659 - accuracy: 0.5598\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6683 - accuracy: 0.5704\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6432 - accuracy: 0.5801\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6339 - accuracy: 0.6225\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6777 - accuracy: 0.5875\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6703 - accuracy: 0.5395\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6562 - accuracy: 0.5574\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6341 - accuracy: 0.6143\n",
      "62/62 [==============================] - 0s 777us/step - loss: 0.9001 - accuracy: 0.4951\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 869us/step - loss: 0.6986 - accuracy: 0.5948\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 961us/step - loss: 0.6468 - accuracy: 0.6282\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6283 - accuracy: 0.6640\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6496 - accuracy: 0.6298\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6153 - accuracy: 0.6566\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5923 - accuracy: 0.6998\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5818 - accuracy: 0.7144\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.7038\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6005 - accuracy: 0.7087\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.7323\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5544 - accuracy: 0.7429\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.7347\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.7307\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5638 - accuracy: 0.7347\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5640 - accuracy: 0.7225\n",
      "62/62 [==============================] - 0s 901us/step - loss: 0.6039 - accuracy: 0.6840\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 863us/step - loss: 0.7688 - accuracy: 0.5419\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 985us/step - loss: 0.6892 - accuracy: 0.5354\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7043 - accuracy: 0.5248\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7065 - accuracy: 0.5663\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6678 - accuracy: 0.5850\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6249 - accuracy: 0.6420\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6055 - accuracy: 0.6241\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6034 - accuracy: 0.6550\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5960 - accuracy: 0.6778\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5856 - accuracy: 0.6542\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.6542\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5702 - accuracy: 0.6688\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.6493\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6516 - accuracy: 0.5956\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5904 - accuracy: 0.6615\n",
      "62/62 [==============================] - 0s 762us/step - loss: 0.7532 - accuracy: 0.5700\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 1ms/step - loss: 0.6080 - accuracy: 0.6564\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.7419\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5146 - accuracy: 0.7410\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7744\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 982us/step - loss: 0.4489 - accuracy: 0.7972\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.8151\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4045 - accuracy: 0.8168\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 989us/step - loss: 0.3781 - accuracy: 0.8339\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8404\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8583\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8607\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.8607\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8770\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2728 - accuracy: 0.8787\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.8836\n",
      "62/62 [==============================] - 0s 801us/step - loss: 0.8840 - accuracy: 0.7305\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 872us/step - loss: 0.6176 - accuracy: 0.6631\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 948us/step - loss: 0.5569 - accuracy: 0.7193\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 960us/step - loss: 0.5087 - accuracy: 0.7608\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 915us/step - loss: 0.4816 - accuracy: 0.7681\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.7917\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 945us/step - loss: 0.4345 - accuracy: 0.7966\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4119 - accuracy: 0.8177\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8299\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8324\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8487\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3497 - accuracy: 0.8544\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.8495\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3157 - accuracy: 0.8568\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8584\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8755\n",
      "62/62 [==============================] - 0s 769us/step - loss: 0.5954 - accuracy: 0.7329\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 822us/step - loss: 0.6005 - accuracy: 0.6859\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 947us/step - loss: 0.5362 - accuracy: 0.7225\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 975us/step - loss: 0.4992 - accuracy: 0.7551\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 986us/step - loss: 0.4697 - accuracy: 0.7852\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.7974\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8104\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8242\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8446\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8519\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8641\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3076 - accuracy: 0.8706\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2914 - accuracy: 0.8836\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8828\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.8934\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2564 - accuracy: 0.9007\n",
      "62/62 [==============================] - 0s 850us/step - loss: 0.9051 - accuracy: 0.6938\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 879us/step - loss: 0.6153 - accuracy: 0.6753\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 946us/step - loss: 0.5373 - accuracy: 0.7323\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 975us/step - loss: 0.5045 - accuracy: 0.7657\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.7844\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7893\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 957us/step - loss: 0.4288 - accuracy: 0.8063\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8202\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8365\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8470\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3446 - accuracy: 0.8397\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8617\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 976us/step - loss: 0.3158 - accuracy: 0.8641\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3021 - accuracy: 0.8714\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3048 - accuracy: 0.8755\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8804\n",
      "62/62 [==============================] - 0s 854us/step - loss: 0.7530 - accuracy: 0.7492\n",
      "Epoch 1/15\n",
      "246/246 [==============================] - 1s 872us/step - loss: 0.6220 - accuracy: 0.6631\n",
      "Epoch 2/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5401 - accuracy: 0.7266\n",
      "Epoch 3/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5036 - accuracy: 0.7518\n",
      "Epoch 4/15\n",
      "246/246 [==============================] - 0s 991us/step - loss: 0.4761 - accuracy: 0.7828\n",
      "Epoch 5/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4472 - accuracy: 0.7836\n",
      "Epoch 6/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.8072\n",
      "Epoch 7/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8348\n",
      "Epoch 8/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8389\n",
      "Epoch 9/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3447 - accuracy: 0.8487\n",
      "Epoch 10/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3178 - accuracy: 0.8690\n",
      "Epoch 11/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8714\n",
      "Epoch 12/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2822 - accuracy: 0.8853\n",
      "Epoch 13/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2744 - accuracy: 0.8877\n",
      "Epoch 14/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.8942\n",
      "Epoch 15/15\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.2349 - accuracy: 0.9048\n",
      "62/62 [==============================] - 0s 779us/step - loss: 0.7651 - accuracy: 0.7720\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 1.2383 - accuracy: 0.5106\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6498 - accuracy: 0.5725\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.7036\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5466 - accuracy: 0.7476\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5435 - accuracy: 0.7329\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7712\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.7752\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4733 - accuracy: 0.7834\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.7858\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7834\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7801\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7875\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.7915\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4560 - accuracy: 0.7964\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7899\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.7907\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.7923\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.8046\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7842\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4449 - accuracy: 0.7980\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.7997\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.7964\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8160\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.8184\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8119\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8160\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8249\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.7997\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8265\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8290\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8604 - accuracy: 0.7078\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 1.3102 - accuracy: 0.5492\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.6810\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.7201\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5457 - accuracy: 0.7356\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7437\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.7697\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7648\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.7893\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7754\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7998\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.8055\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.8072\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.8055\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8063\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8161\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8275\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4138 - accuracy: 0.8275\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8259\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8308\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8332\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8251\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.8381\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3879 - accuracy: 0.8316\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8340\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.8397\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.8365\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3419 - accuracy: 0.8576\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8568\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8511\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.8690\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9011 - accuracy: 0.7003\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 1.2344 - accuracy: 0.4980\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.6119\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.7030\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.7177\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7559\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.7689\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7673\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.7697\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.7730\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7681\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7583\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7583\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7722\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7819\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7836\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7852\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.7705\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.7852\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7836\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7868\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7917\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7876\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7893\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7909\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.7941\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7746\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.7738\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7844\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.7925\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.7893\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8327 - accuracy: 0.6808\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 1.0087 - accuracy: 0.5207\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.5972\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.7225\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7201\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.7600\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7697\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7852\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.7901\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.8007\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.8088\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8055\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8080\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8153\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8267\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8055\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8031\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8275\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3858 - accuracy: 0.8218\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8104\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3801 - accuracy: 0.8267\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.8218\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3634 - accuracy: 0.8267\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3581 - accuracy: 0.8389\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8462\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8340\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.8275\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3665 - accuracy: 0.8283\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8397\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8397\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8332\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0001 - accuracy: 0.7068\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 0.9871 - accuracy: 0.5631\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.6737\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.6957\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5548 - accuracy: 0.7388\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7559\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7453\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7624\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7535\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7665\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7779\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7738\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.7640\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4374 - accuracy: 0.7795\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7705\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.7811\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.7828\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7681\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.7852\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4160 - accuracy: 0.7860\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.7787\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.7868\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.7950\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.7738\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.7884\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.7868\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.7762\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.7819\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.7844\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.7893\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.7803\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.7231\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 957us/step - loss: 0.7001 - accuracy: 0.6042\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 954us/step - loss: 0.6600 - accuracy: 0.5863\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6508 - accuracy: 0.6336\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 994us/step - loss: 0.6355 - accuracy: 0.6230\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6590 - accuracy: 0.6002\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 974us/step - loss: 0.6410 - accuracy: 0.5985\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6130 - accuracy: 0.6425\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 997us/step - loss: 0.6215 - accuracy: 0.6360\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6088 - accuracy: 0.6645\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6340 - accuracy: 0.6336\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6153 - accuracy: 0.6164\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5989 - accuracy: 0.6441\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6023 - accuracy: 0.6490\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5981 - accuracy: 0.6531\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6026 - accuracy: 0.6621\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5933 - accuracy: 0.6555\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5826 - accuracy: 0.6596\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 989us/step - loss: 0.6254 - accuracy: 0.6319\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6032 - accuracy: 0.6401\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6062 - accuracy: 0.6490\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6074 - accuracy: 0.6401\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5896 - accuracy: 0.6482\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5950 - accuracy: 0.6515\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5908 - accuracy: 0.6547\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5838 - accuracy: 0.6507\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5872 - accuracy: 0.6515\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5920 - accuracy: 0.6539\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5905 - accuracy: 0.6564\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6106 - accuracy: 0.6482\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5901 - accuracy: 0.6637\n",
      "62/62 [==============================] - 0s 924us/step - loss: 0.6731 - accuracy: 0.6136\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 923us/step - loss: 0.7073 - accuracy: 0.5997\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 956us/step - loss: 0.7145 - accuracy: 0.5549\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6689 - accuracy: 0.6046\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6606 - accuracy: 0.5964\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6384 - accuracy: 0.6534\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6016 - accuracy: 0.6648\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6625 - accuracy: 0.6249\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6308 - accuracy: 0.6493\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6432 - accuracy: 0.6338\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6182 - accuracy: 0.6404\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6151 - accuracy: 0.6477\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5965 - accuracy: 0.6859\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6142 - accuracy: 0.6729\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6032 - accuracy: 0.6648\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6180 - accuracy: 0.6509\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6162 - accuracy: 0.6444\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 988us/step - loss: 0.6087 - accuracy: 0.6420\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 976us/step - loss: 0.5685 - accuracy: 0.6973\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5655 - accuracy: 0.7046\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.6843\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5929 - accuracy: 0.6753\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5804 - accuracy: 0.6859\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5752 - accuracy: 0.6941\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5658 - accuracy: 0.6827\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5555 - accuracy: 0.7063\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7030\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5536 - accuracy: 0.7022\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5481 - accuracy: 0.6941\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.7055\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5722 - accuracy: 0.6859\n",
      "62/62 [==============================] - 0s 916us/step - loss: 1.7394 - accuracy: 0.6678\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 857us/step - loss: 0.6574 - accuracy: 0.6338\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 979us/step - loss: 0.5942 - accuracy: 0.7014\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 988us/step - loss: 0.5889 - accuracy: 0.7185\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6024 - accuracy: 0.7055\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5471 - accuracy: 0.7396\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5764 - accuracy: 0.7258\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5724 - accuracy: 0.7290\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5465 - accuracy: 0.7274\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5574 - accuracy: 0.7030\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5384 - accuracy: 0.7469\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 998us/step - loss: 0.5758 - accuracy: 0.7242\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.7502\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5135 - accuracy: 0.7640\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5389 - accuracy: 0.7315\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5201 - accuracy: 0.7445\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5474 - accuracy: 0.6867\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5137 - accuracy: 0.7494\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5267 - accuracy: 0.7510\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5291 - accuracy: 0.7201\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5168 - accuracy: 0.7461\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5159 - accuracy: 0.7624\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5337 - accuracy: 0.7201\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7510\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5142 - accuracy: 0.7445\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5121 - accuracy: 0.7478\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4943 - accuracy: 0.7648\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4938 - accuracy: 0.7307\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5080 - accuracy: 0.7486\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.4786 - accuracy: 0.7437\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7225\n",
      "62/62 [==============================] - 0s 799us/step - loss: 1.4111 - accuracy: 0.6743\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 847us/step - loss: 0.6959 - accuracy: 0.6208\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 888us/step - loss: 0.6765 - accuracy: 0.5728\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 927us/step - loss: 0.6499 - accuracy: 0.6298\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6306 - accuracy: 0.6566\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6736 - accuracy: 0.6054\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6048 - accuracy: 0.6672\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6155 - accuracy: 0.6623\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5972 - accuracy: 0.6705\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7030\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5957 - accuracy: 0.6623\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6660 - accuracy: 0.5972\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 981us/step - loss: 0.5858 - accuracy: 0.6436\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5516 - accuracy: 0.6884\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.6941\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.7030\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5551 - accuracy: 0.7120\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.6827\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6130 - accuracy: 0.6672\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5650 - accuracy: 0.7136\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7030\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5629 - accuracy: 0.6835\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.6981\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.7217\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5894 - accuracy: 0.6583\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6205 - accuracy: 0.6908\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.6884\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6064 - accuracy: 0.6151\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5672 - accuracy: 0.6884\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5575 - accuracy: 0.6876\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.6851\n",
      "62/62 [==============================] - 0s 899us/step - loss: 0.8067 - accuracy: 0.6612\n",
      "Epoch 1/30\n",
      "246/246 [==============================] - 1s 841us/step - loss: 0.7160 - accuracy: 0.4939\n",
      "Epoch 2/30\n",
      "246/246 [==============================] - 0s 999us/step - loss: 0.6983 - accuracy: 0.5175\n",
      "Epoch 3/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6967 - accuracy: 0.4972\n",
      "Epoch 4/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7025 - accuracy: 0.4980\n",
      "Epoch 5/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.4809\n",
      "Epoch 6/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6987 - accuracy: 0.4866\n",
      "Epoch 7/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6975 - accuracy: 0.4915\n",
      "Epoch 8/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6976 - accuracy: 0.4866\n",
      "Epoch 9/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7028 - accuracy: 0.5069\n",
      "Epoch 10/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7011 - accuracy: 0.4923\n",
      "Epoch 11/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6953 - accuracy: 0.5118\n",
      "Epoch 12/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6975 - accuracy: 0.5028\n",
      "Epoch 13/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6963 - accuracy: 0.5191\n",
      "Epoch 14/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6974 - accuracy: 0.5151\n",
      "Epoch 15/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6977 - accuracy: 0.5094\n",
      "Epoch 16/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.4906\n",
      "Epoch 17/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6964 - accuracy: 0.5134\n",
      "Epoch 18/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6990 - accuracy: 0.4988\n",
      "Epoch 19/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6979 - accuracy: 0.5037\n",
      "Epoch 20/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6991 - accuracy: 0.5012\n",
      "Epoch 21/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6988 - accuracy: 0.5037\n",
      "Epoch 22/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6970 - accuracy: 0.5118\n",
      "Epoch 23/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6991 - accuracy: 0.4980\n",
      "Epoch 24/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7013 - accuracy: 0.4614\n",
      "Epoch 25/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.4890\n",
      "Epoch 26/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.4841\n",
      "Epoch 27/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6992 - accuracy: 0.5053\n",
      "Epoch 28/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6980 - accuracy: 0.5175\n",
      "Epoch 29/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6997 - accuracy: 0.5028\n",
      "Epoch 30/30\n",
      "246/246 [==============================] - 0s 1ms/step - loss: 0.6983 - accuracy: 0.4906\n",
      "62/62 [==============================] - 0s 883us/step - loss: 0.6933 - accuracy: 0.4951\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 926us/step - loss: 0.7492 - accuracy: 0.5863\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 929us/step - loss: 0.6826 - accuracy: 0.5928\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7026 - accuracy: 0.5741\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6604 - accuracy: 0.6067\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6647 - accuracy: 0.5635\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6853 - accuracy: 0.6181\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6884 - accuracy: 0.5643\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6555 - accuracy: 0.5765\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6766 - accuracy: 0.5871\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6409 - accuracy: 0.5733\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6396 - accuracy: 0.6083\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6531 - accuracy: 0.5806\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7268 - accuracy: 0.5432\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6733 - accuracy: 0.5480\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6535 - accuracy: 0.5660\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6365 - accuracy: 0.6213\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6553 - accuracy: 0.5798\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6439 - accuracy: 0.6287\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6041 - accuracy: 0.6303\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6146 - accuracy: 0.6344\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6091 - accuracy: 0.6246\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6023 - accuracy: 0.6417\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5882 - accuracy: 0.6466\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.6099\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.5968 - accuracy: 0.6376\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6134 - accuracy: 0.6254\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6335 - accuracy: 0.5635\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6178 - accuracy: 0.6197\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6224 - accuracy: 0.6254\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6134 - accuracy: 0.6099\n",
      "31/31 [==============================] - 0s 897us/step - loss: 1.0829 - accuracy: 0.5682\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 867us/step - loss: 0.7842 - accuracy: 0.5126\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7018 - accuracy: 0.5037\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7441 - accuracy: 0.5118\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4744\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4825\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6997 - accuracy: 0.5118\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7038 - accuracy: 0.4801\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6994 - accuracy: 0.5028\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7024 - accuracy: 0.4923\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6981 - accuracy: 0.4915\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.5134\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 986us/step - loss: 0.7015 - accuracy: 0.5020\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.4915\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7012 - accuracy: 0.4963\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6986 - accuracy: 0.4898\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6991 - accuracy: 0.4898\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7012 - accuracy: 0.5126\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.4776\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7031 - accuracy: 0.5069\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7091 - accuracy: 0.4784\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6984 - accuracy: 0.4825\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7063 - accuracy: 0.5004\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7045 - accuracy: 0.4841\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7006 - accuracy: 0.5199\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7003 - accuracy: 0.5053\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.5020\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6986 - accuracy: 0.4996\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6992 - accuracy: 0.5159\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7009 - accuracy: 0.4825\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.5077\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.7090 - accuracy: 0.4951\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 844us/step - loss: 0.7371 - accuracy: 0.5240\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 954us/step - loss: 0.6989 - accuracy: 0.4955\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6954 - accuracy: 0.5020\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 997us/step - loss: 0.6900 - accuracy: 0.5273\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5085\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6961 - accuracy: 0.5232\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6937 - accuracy: 0.4858\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6956 - accuracy: 0.4980\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.5256\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5264\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.4906\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.5273\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5159\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.5028\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6936 - accuracy: 0.5020\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5281\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.4931\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6866 - accuracy: 0.5264: 0s - loss: 0.6864 - accuracy: 0.53\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6947 - accuracy: 0.5167\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.5216\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6970 - accuracy: 0.4923\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.4947\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6960 - accuracy: 0.5053\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6925 - accuracy: 0.4784\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5004\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.5037\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.5183\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.5004\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6846 - accuracy: 0.5346\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 997us/step - loss: 0.9055 - accuracy: 0.4951\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 922us/step - loss: 0.7238 - accuracy: 0.6282\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6351 - accuracy: 0.6550\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 997us/step - loss: 0.6266 - accuracy: 0.6778\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 965us/step - loss: 0.6532 - accuracy: 0.6282\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6648 - accuracy: 0.6029\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6491 - accuracy: 0.6143\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6785 - accuracy: 0.5956\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6970 - accuracy: 0.5102\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.4809\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7033 - accuracy: 0.4874\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7014 - accuracy: 0.4972\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4915\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6985 - accuracy: 0.4825\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6999 - accuracy: 0.5012\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6984 - accuracy: 0.4890\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6958 - accuracy: 0.4931\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.4963\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6957 - accuracy: 0.5183\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7005 - accuracy: 0.4882\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6986 - accuracy: 0.4931\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.5281\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.5012\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.4980\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.5028\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6974 - accuracy: 0.5085\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7010 - accuracy: 0.4809\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6998 - accuracy: 0.5134\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7030 - accuracy: 0.5004\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7053 - accuracy: 0.5232\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.4963\n",
      "31/31 [==============================] - 0s 997us/step - loss: 0.6950 - accuracy: 0.4951\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 1s 1ms/step - loss: 0.7304 - accuracy: 0.5175\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7299 - accuracy: 0.5102\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6882 - accuracy: 0.5167\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5273\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6953 - accuracy: 0.4988\n",
      "Epoch 6/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5191\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5338\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6961 - accuracy: 0.4890\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7023 - accuracy: 0.5028\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 0s 979us/step - loss: 0.6904 - accuracy: 0.5061\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7036 - accuracy: 0.4955\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6937 - accuracy: 0.5094\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5053\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6959 - accuracy: 0.5020\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6975 - accuracy: 0.4679\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6940 - accuracy: 0.5248\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6995 - accuracy: 0.5045\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6977 - accuracy: 0.5020\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6974 - accuracy: 0.4793\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6958 - accuracy: 0.5094\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5297\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6994 - accuracy: 0.4793\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7024 - accuracy: 0.4809\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7025 - accuracy: 0.4768\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7013 - accuracy: 0.4711\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6983 - accuracy: 0.4817\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6940 - accuracy: 0.5045\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7003 - accuracy: 0.5061\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.6950 - accuracy: 0.5134\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.4955\n",
      "31/31 [==============================] - 0s 797us/step - loss: 0.6943 - accuracy: 0.5049\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 0.6366 - accuracy: 0.6262\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7174\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.7500\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.7752\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.7932\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4193 - accuracy: 0.8103\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8160\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8428\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3427 - accuracy: 0.8502\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8705\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.8884\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8836\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9104\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.9104\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9251\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.9259\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9283\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1846 - accuracy: 0.9389\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2040 - accuracy: 0.9210\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1959 - accuracy: 0.9259\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1611 - accuracy: 0.9463\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9487\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.9625\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.9577\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9666\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9634\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1135 - accuracy: 0.9674\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1103 - accuracy: 0.9691\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9674\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0939 - accuracy: 0.9756\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9972 - accuracy: 0.7338\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 0.6392 - accuracy: 0.6347\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.7185\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7600\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7771\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.7950\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8063\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8299\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8308\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3719 - accuracy: 0.8430\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8641\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8519\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8779\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8869\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8609\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.9040\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.9024\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2451 - accuracy: 0.9211\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2208 - accuracy: 0.9260\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2090 - accuracy: 0.9325\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9251\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1991 - accuracy: 0.9317\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1850 - accuracy: 0.9349\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1781 - accuracy: 0.9430\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1683 - accuracy: 0.9447\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1556 - accuracy: 0.9439\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9512\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9626\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9593\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9585\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.9552\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6741 - accuracy: 0.7655\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 0.6239 - accuracy: 0.6420\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7331\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7632\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7665\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.7941\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8169\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.8324\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8365\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8600\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8592\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8853\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.8706\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2679 - accuracy: 0.8934\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.8910\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9113\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9219\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9284\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.9325\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9349\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1830 - accuracy: 0.9341\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1761 - accuracy: 0.9373\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1685 - accuracy: 0.9406\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9544\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.9544\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9585\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9601\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9634\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1102 - accuracy: 0.9715\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9642\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 0.9601\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9484 - accuracy: 0.7492\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6558\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7160\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7461\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7795\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.7950\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.7950\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8259\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8365\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3622 - accuracy: 0.8430\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8576\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8625\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8828\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8755\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8633\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8779\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8885\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.9138\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9170\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9243\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.9170\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.9227\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.9406\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1654 - accuracy: 0.9373\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9479\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9430\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9544\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.9528\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9528\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9593\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9618\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9127 - accuracy: 0.7524\n",
      "Epoch 1/30\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 0.6284 - accuracy: 0.6241\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7234\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.7551\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.7803\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.7958\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8120\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8421\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8397\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8625\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8779\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.8885\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.8983\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.9056\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2241 - accuracy: 0.9170\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9219\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2165 - accuracy: 0.9203\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1970 - accuracy: 0.9284\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.9357\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1682 - accuracy: 0.9447\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1563 - accuracy: 0.9504\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9536\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9642\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9626\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9585\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.9593\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1409 - accuracy: 0.9528\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9658\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1061 - accuracy: 0.9675\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1035 - accuracy: 0.9675\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9740\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7432 - accuracy: 0.7850\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 1s 2ms/step - loss: 0.6259 - accuracy: 0.6387\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5514 - accuracy: 0.7090\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7467\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.7812\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7930\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.7962\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.8203\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3749 - accuracy: 0.8385\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8509\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8672\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3007 - accuracy: 0.8776\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8757\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8783\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.9017\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.9049\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.9193\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9225\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1911 - accuracy: 0.9336\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9395\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9414\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1630 - accuracy: 0.9525\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.9518\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1541 - accuracy: 0.9473\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1432 - accuracy: 0.9570\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1461 - accuracy: 0.9505\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1362 - accuracy: 0.9577\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9577\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9616\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9707\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.9727\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#fit the nn_model with X_train and y_train data\n",
    "nn_model = rscv_nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by randomised search are: {'clf__unit': 30, 'clf__learn_rate': 0.01, 'clf__epochs': 30, 'clf__batch_size': 100}\n",
      "Best accuracy from randomised search is: 0.7571767091751098\n"
     ]
    }
   ],
   "source": [
    "#print best parameters and accuracy\n",
    "print ('Best parameters found by randomised search are:', nn_model.best_params_)\n",
    "\n",
    "print ('Best accuracy from randomised search is:', nn_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Refer to following link on how to save and load a keras model that is wrapped in sklearn**</font>\n",
    "\n",
    "https://stackoverflow.com/questions/37984304/how-to-save-a-scikit-learn-pipline-with-keras-regressor-inside-to-disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Saving the model and pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x17039af2640>"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ensure the model is retrieved via the named steps in randomised search\n",
    "rscv_nn.best_estimator_.named_steps['clf'].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Keras model first:\n",
    "rscv_nn.best_estimator_.named_steps['clf'].model.save('fy21_rscv_nn_model_binarygoodnotgood.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This hack emptying the model allows us to save just the sklearn pipeline object:\n",
    "rscv_nn.best_estimator_.named_steps['clf'].model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fy21_nn_model_pipeline_binarygoodnotgood_v3.pkl']"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, save the pipeline using joblib\n",
    "joblib.dump(rscv_nn.best_estimator_, 'fy21_rscv_nn_model_pipeline_binarygoodnotgood.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Loading the pipeline, and then the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the pipeline first:\n",
    "rscv_nn.best_estimator_ = joblib.load('fy21_rscv_nn_model_pipeline_binarygoodnotgood.pkl')\n",
    "\n",
    "# Then, load the Keras model:\n",
    "rscv_nn.best_estimator_.named_steps['clf'].model = load_model('fy21_rscv_nn_model_binarygoodnotgood.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 5s 985us/step\n"
     ]
    }
   ],
   "source": [
    "#predict the test data\n",
    "y_pred_nn = rscv_nn.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.73       260\n",
      "           1       0.72      0.73      0.73       252\n",
      "\n",
      "    accuracy                           0.73       512\n",
      "   macro avg       0.73      0.73      0.73       512\n",
      "weighted avg       0.73      0.73      0.73       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_test,y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 995us/step\n",
      "Wall time: 21 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actual prediction with validation data\n",
    "\n",
    "actual_pred_nn = rscv_nn.best_estimator_.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       260\n",
      "           1       0.78      0.74      0.76       252\n",
      "\n",
      "    accuracy                           0.77       512\n",
      "   macro avg       0.77      0.77      0.77       512\n",
      "weighted avg       0.77      0.77      0.77       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect classification metrics\n",
    "print(metrics.classification_report(y_val,actual_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predicted model\n",
    "pkl_filename = 'fy21_nnpredictionmodel.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(actual_pred_nn, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, actual_pred_nn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "ax.yaxis.set_ticklabels(['Not Good', 'Good'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8255 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8694 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9059 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9018 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9266 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8976 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8798 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9057 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9464 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8994 - accuracy: 0.7500\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7867 - accuracy: 0.79 - 0s 997us/step - loss: 0.9352 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9194 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9066 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9176 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8834 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9545 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9118 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9394 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8799 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9042 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9487 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9213 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9364 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9115 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9266 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9225 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9100 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9397 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8718 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8969 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.9020 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7646 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8428 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8022 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8692 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7926 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7253 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8617 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8062 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7945 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7785 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8130 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7878 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8239 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7568 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7560 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7495 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7986 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8054 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8621 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8165 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8325 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.7719 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7817 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8112 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7818 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8360 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8133 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7936 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7851 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8235 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8577 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8916 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8820 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9461 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8505 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9406 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8689 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8892 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9265 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9026 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9016 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9056 - accuracy: 0.7422\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7318 - accuracy: 0.80 - 0s 1ms/step - loss: 0.9206 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8848 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9342 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8967 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8947 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8809 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8976 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9699 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9096 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9104 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8549 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9021 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8902 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8672 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8702 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8606 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9103 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9197 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8828 - accuracy: 0.7266\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6645 - accuracy: 0.72 - 0s 2ms/step - loss: 0.8679 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8336 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9025 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8812 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8664 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8322 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8559 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8424 - accuracy: 0.7383\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7648 - accuracy: 0.75 - 0s 2ms/step - loss: 0.8757 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8852 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8499 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8694 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8173 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8871 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9064 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8572 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8124 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8456 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8391 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8429 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8232 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8552 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8349 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8284 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8625 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8755 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8553 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8645 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8715 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8248 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7773 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7860 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8340 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7958 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8075 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7435 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7948 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8146 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8129 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8367 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7723 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8119 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8267 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8072 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8196 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8371 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8084 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7812 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7783 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8140 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7787 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8110 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7810 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8252 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8294 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8033 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8125 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8276 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7881 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8034 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8039 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8510 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7799 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8257 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8173 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7667 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8203 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8498 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8302 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8407 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7694 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8336 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8286 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8173 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8612 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7944 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8163 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8185 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7942 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8197 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.7901 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8036 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8282 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.7581 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.7897 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8098 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7907 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7993 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7903 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8835 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8923 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8991 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9069 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8799 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8872 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 998us/step - loss: 0.8606 - accuracy: 0.7344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9070 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9151 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8688 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8934 - accuracy: 0.7266\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8776 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9254 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8940 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9401 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9276 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9098 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8420 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8453 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8823 - accuracy: 0.7266\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8696 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8665 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9175 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8694 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9346 - accuracy: 0.7266\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8466 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9253 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8670 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8770 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8308 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8309 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8280 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8334 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8267 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8362 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8408 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8326 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8070 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8098 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8274 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8369 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8443 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8097 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8570 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8716 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8468 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8229 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8317 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8550 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8106 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8248 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7852 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8211 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8332 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8465 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8208 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8344 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8106 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8434 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9157 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8754 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8701 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8567 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8828 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8648 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8823 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8753 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8989 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8879 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8797 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8717 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9004 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8360 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9012 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.9017 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.8692 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8784 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8732 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8413 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8988 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8824 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8392 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8863 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8853 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9346 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9157 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8884 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8836 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8292 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8153 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8246 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8189 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8138 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8411 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8121 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8467 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8452 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8475 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8071 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8344 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8402 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8325 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8292 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8194 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8102 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8212 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8342 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8315 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8035 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8266 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8138 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8382 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8490 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8408 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8330 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8081 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8245 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8328 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8590 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9613 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9538 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9522 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9779 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9396 - accuracy: 0.7012\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9800 - accuracy: 0.7012\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9636 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9986 - accuracy: 0.7109\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8960 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0298 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9699 - accuracy: 0.7109\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9826 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9350 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8867 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9605 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9670 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9473 - accuracy: 0.7070\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9289 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0583 - accuracy: 0.6934\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9981 - accuracy: 0.7070\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9981 - accuracy: 0.7012\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9525 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9887 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9509 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9460 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9729 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9353 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9473 - accuracy: 0.7070\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9468 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9686 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8327 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8190 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8389 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8439 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8350 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8650 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8449 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8480 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8500 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8224 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8499 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8224 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8318 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8093 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8162 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8435 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8247 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8339 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8316 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8586 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8114 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8572 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8741 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8633 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8507 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8537 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8051 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8338 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8397 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8405 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8205 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8448 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8643 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8441 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8410 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8507 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8475 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8568 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8363 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8410 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8253 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8425 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8553 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8206 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8508 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8360 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8729 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8442 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8392 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8838 - accuracy: 0.7598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7977 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8286 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8569 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8494 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8681 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8598 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8214 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8313 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8701 - accuracy: 0.7812\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8250 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8509 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9317 - accuracy: 0.7109\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9501 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9595 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9103 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8292 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8803 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9095 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9015 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8733 - accuracy: 0.7266\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8678 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8671 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9341 - accuracy: 0.7148\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8912 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8973 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8533 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8151 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9074 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9173 - accuracy: 0.7266\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9426 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9420 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9171 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8620 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9222 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8366 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9487 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8946 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8704 - accuracy: 0.7285\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8355 - accuracy: 0.7305\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9075 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8520 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8497 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8442 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8427 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8271 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8501 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8377 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8482 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8509 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8557 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8525 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8626 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8633 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8784 - accuracy: 0.7402\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8468 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8442 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8843 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8657 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8525 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8718 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8493 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8454 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8654 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8514 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8705 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8327 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8451 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8718 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8441 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8433 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8458 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8712 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8629 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8376 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8623 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8470 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8738 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8626 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8559 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8432 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8640 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8635 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8383 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8411 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8728 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8787 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8711 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8454 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8511 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8739 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9108 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8583 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8616 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8628 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8802 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8722 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8648 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8569 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8579 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8180 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8170 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8190 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8161 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8230 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8199 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8198 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8233 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8169 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8196 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8213 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8176 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8219 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8231 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8205 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8193 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8144 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8213 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8165 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8215 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8214 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8199 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8169 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8182 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8196 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8195 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8170 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8181 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8196 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8190 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8222 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8213 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8193 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8193 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8111 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8149 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8131 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8067 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8190 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8061 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8163 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8263 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8169 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8288 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8236 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8268 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8175 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8168 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8046 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8155 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8230 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8196 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8192 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8242 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8069 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8049 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8106 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8171 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8191 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8223 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8322 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8157 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8195 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8288 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8217 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8268 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8284 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8222 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8203 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8258 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8310 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8346 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8274 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8300 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8239 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8220 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8178 - accuracy: 0.7793\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8260 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8200 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8262 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8218 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8244 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8236 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8271 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8233 - accuracy: 0.7773\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8293 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8169 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8228 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8201 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8239 - accuracy: 0.7676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8261 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8202 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8183 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8248 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8226 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8194 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8242 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8219 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8215 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8190 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8224 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8191 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8212 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8203 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8135 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8199 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8182 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8196 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8164 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8278 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8201 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8247 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8187 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8168 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8164 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8229 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8210 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8251 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8267 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8278 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8356 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8246 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8295 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8231 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8240 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8243 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8274 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8296 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8245 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8271 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8267 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8288 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8285 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8286 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8274 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8248 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8274 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8287 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8275 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8302 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8259 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8275 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8256 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8281 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8247 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8346 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8274 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8297 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8429 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8569 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8489 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8391 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8483 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8403 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8492 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8417 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8311 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8379 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8421 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8230 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8641 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8643 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8667 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8396 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8508 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8398 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8220 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8257 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8333 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8499 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8468 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8489 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8259 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8412 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8582 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8343 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8231 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8462 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8501 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8454 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8354 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8473 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8471 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8384 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8581 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8321 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8491 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8128 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8362 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8300 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8497 - accuracy: 0.7773\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8598 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8323 - accuracy: 0.7793\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8249 - accuracy: 0.7812\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8366 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8542 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8393 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8435 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8370 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8344 - accuracy: 0.7832\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8355 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8305 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8556 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8399 - accuracy: 0.7773\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8260 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8542 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8350 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8253 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8271 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8454 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8324 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8322 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8490 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8426 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8390 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8475 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8481 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8346 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8413 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8431 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8325 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8417 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8168 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8434 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8415 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8272 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8322 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8495 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8199 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8337 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8314 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8353 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8479 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8481 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8271 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8394 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8299 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8291 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8287 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8299 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8317 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8282 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8199 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8291 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8092 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8231 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8281 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8105 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8245 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8242 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8313 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8146 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8177 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8315 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8309 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8308 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8260 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8243 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8180 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8217 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8216 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8308 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8256 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8272 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8300 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8391 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8290 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8269 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8183 - accuracy: 0.7812\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8378 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8287 - accuracy: 0.7773\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8287 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8234 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8324 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8247 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8454 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8316 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8312 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8275 - accuracy: 0.7695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8239 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8117 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8227 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8268 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8273 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8342 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8345 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8397 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8224 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8173 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8147 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8412 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8228 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8183 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8406 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8287 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8232 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8218 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8143 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8153 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8202 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8080 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8240 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8127 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8122 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8218 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8233 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8165 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8251 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7995 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8147 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8116 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8136 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8184 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8158 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8185 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8285 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8131 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8072 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8177 - accuracy: 0.7754\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8138 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8157 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8142 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8164 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8258 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8321 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8193 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8350 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8320 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8304 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8398 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8348 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8314 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8353 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8380 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8166 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8314 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8373 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8199 - accuracy: 0.7734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8278 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8217 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8392 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8379 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8229 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8259 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8317 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8401 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8319 - accuracy: 0.7676\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8295 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8312 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8335 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8378 - accuracy: 0.7637\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8170 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9914 - accuracy: 0.6973\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9536 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9762 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0225 - accuracy: 0.6875\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9296 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0453 - accuracy: 0.7012\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0090 - accuracy: 0.6875\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0273 - accuracy: 0.6875\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0112 - accuracy: 0.7051\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9775 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0127 - accuracy: 0.7090\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9440 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9472 - accuracy: 0.7266\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9512 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9845 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9600 - accuracy: 0.6953\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9967 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9514 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0019 - accuracy: 0.7051\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9840 - accuracy: 0.7012\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9848 - accuracy: 0.7051\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0239 - accuracy: 0.6816\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9317 - accuracy: 0.7109\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0325 - accuracy: 0.6895\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9691 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9466 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9713 - accuracy: 0.7070\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9387 - accuracy: 0.7207\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9874 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9807 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8899 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8541 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9025 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9154 - accuracy: 0.7168\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8832 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8916 - accuracy: 0.7344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9400 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9177 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9268 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9393 - accuracy: 0.7051\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9304 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9002 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9273 - accuracy: 0.7129\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9306 - accuracy: 0.7012\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9405 - accuracy: 0.7109\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9084 - accuracy: 0.7246\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9459 - accuracy: 0.7109\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8894 - accuracy: 0.7324\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9402 - accuracy: 0.7031\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9058 - accuracy: 0.7266\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9263 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8993 - accuracy: 0.7363\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9271 - accuracy: 0.7109\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9600 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9302 - accuracy: 0.7227\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9192 - accuracy: 0.7148\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9071 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9051 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9175 - accuracy: 0.7070\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9053 - accuracy: 0.7148\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8680 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8703 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8337 - accuracy: 0.7598\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8579 - accuracy: 0.7461\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8460 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8500 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8703 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8520 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8818 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8549 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8590 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8649 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8399 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8701 - accuracy: 0.7383\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8478 - accuracy: 0.7656\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8316 - accuracy: 0.7715\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8855 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8824 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8549 - accuracy: 0.7695\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8398 - accuracy: 0.7441\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8456 - accuracy: 0.7500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8759 - accuracy: 0.7480\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8427 - accuracy: 0.7422\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8852 - accuracy: 0.7578\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8721 - accuracy: 0.7617\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8577 - accuracy: 0.7559\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8770 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8463 - accuracy: 0.7520\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8696 - accuracy: 0.7539\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8431 - accuracy: 0.7578\n",
      "Feature ranking:\n",
      "1. MANAGER180_OVERALL_BAND_CURR_FY (0.058594)\n",
      "2. ATTAINMENT_REP_LEVEL_BAND_PREV_FY (0.048828)\n",
      "3. PRODUCT_ASSOCIATION (0.046354)\n",
      "4. NATIONALITY (0.039779)\n",
      "5. TIME_SINCE_LAST_SALARY_INCR_BAND (0.034310)\n",
      "6. CAREER_LEVEL (0.025977)\n",
      "7. TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN (0.025586)\n",
      "8. AGE_BAND (0.023698)\n",
      "9. MANAGER_GENDER_DESC (0.021159)\n",
      "10. COUNTRY (0.021029)\n",
      "11. RCODE_06 (0.018490)\n",
      "12. SUBREGION (0.017188)\n",
      "13. GENDER (0.015885)\n",
      "14. TENURE_LATEST_HIRE_DATE_BAND_LIN (0.014909)\n",
      "15. MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL (0.014258)\n",
      "16. JOB_TENURE_BAND_LIN (0.013542)\n",
      "17. HIRE_EVENT_DESCRIPTION (0.009831)\n",
      "18. RCODE_07 (0.008984)\n",
      "19. NUM_CAREER_LEVEL_CHANGE_CURR_FY (0.008789)\n",
      "20. PRODUCT_LINE (0.007812)\n",
      "21. NUM_SALARY_CHANGE_PREV_FY (0.003385)\n",
      "22. NUM_MANAGER_CHANGE_CURR_FY (0.002539)\n",
      "23. NUM_ORG_CHANGE_CURR_FY (0.000326)\n",
      "24. NUM_LOCATION_CHANGE_CURR_FY (0.000195)\n",
      "25. NUM_SALARY_CHANGE_CURR_FY (0.000065)\n",
      "26. NUM_CAREER_LEVEL_CHANGE_PREV_FY (-0.000260)\n",
      "27. NUM_MANAGER_CHANGE_PREV_FY (-0.001107)\n",
      "28. NUM_JOB_CHANGE_PREV_FY (-0.001432)\n",
      "29. NUM_LOCATION_CHANGE_PREV_FY (-0.002865)\n",
      "30. NUM_JOB_CHANGE_CURR_FY (-0.003385)\n",
      "31. NUM_ORG_CHANGE_PREV_FY (-0.004427)\n"
     ]
    }
   ],
   "source": [
    "# The Permutation based feature importance is used here\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "#use the permutation importance function on validation data\n",
    "imps = permutation_importance(rscv_nn.best_estimator_, X_val, y_val, n_repeats=30)\n",
    "importances = imps.importances_mean\n",
    "std = imps.importances_std\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_val.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place the features into a dataframe and rename the columns\n",
    "\n",
    "importances2 = list(zip(importances, X.columns))\n",
    "\n",
    "nn_perm_impt = pd.DataFrame(importances2)\n",
    "\n",
    "nn_perm_impt.rename(columns = {0:'Impt Values', 1:'Features'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 10 features\n",
    "\n",
    "nn_perm_top10 = nn_perm_impt.sort_values(by=['Impt Values'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 10 features in descending order\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "features = sns.barplot(x=\"Impt Values\", y=\"Features\", data=nn_perm_top10.sort_values(by=\"Impt Values\", ascending=False), color=\"#2C5967\")\n",
    "plt.title('Shallow NN Permutation Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "figure = features.get_figure()\n",
    "#save the chart for use later\n",
    "figure.savefig('Shallow_nn_features.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 1: Predicting new classes using max voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Max voting allows you to select the most commonly predicted class from the models trained. This helps to increase confidence in the class predicted**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the models\n",
    "from keras.models import load_model\n",
    "\n",
    "with open ('fy21_lr_rscv_binarygoodnotgood_model.pkl','rb') as file1:\n",
    "    lr_model2 = pickle.load(file1)\n",
    "    \n",
    "with open ('fy21_rf_rscv_binarygoodnotgood_model.pkl','rb') as file2:\n",
    "    rf_model = pickle.load(file2)\n",
    "    \n",
    "with open ('fy21_lgbm_rscv_binarygoodnotgood_model.pkl','rb') as file3:\n",
    "    lgbm_model = pickle.load(file3)\n",
    "\n",
    "with open ('fy21_svc_rscv_binarygoodnotgood_model.pkl','rb') as file4:\n",
    "    svc_model = pickle.load(file4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Before loading the pipeline and the keras model, be sure to import the keras libraries, to run the 'create_network' function, to run the randomised search cv and create the nn classifier object. Not doing so would lead to error as the create_network function is not saved in the pipeline nor model**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before loading the pipeline and the keras model\n",
    "#make sure to import the keras libraries, to run the 'create_network' function, and to run the randomised search cv object\n",
    "\n",
    "#load all the models\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pipeline first:\n",
    "rscv_nn.best_estimator_ = joblib.load('fy21_nn_model_pipeline_binarygoodnotgood_v3.pkl')\n",
    "\n",
    "# Then, load the Keras model:\n",
    "rscv_nn.best_estimator_.named_steps['clf'].model = load_model('fy21_nn_model_binarygoodnotgood_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rscv_nn.estimator = joblib.load('fy21_nn_model_pipeline_binarygoodnotgood_test1.pkl')\n",
    "\n",
    "# rscv_nn = joblib.load('fy21_nn_model_pipeline_binarygoodnotgood_test2.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Load in the new data that has been subjected to preprocessing and feature engineering**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change directory and load the dataset\n",
    "os.chdir('C:/Users/weijtan/Desktop/NUS/Capstone/data')\n",
    "\n",
    "df_new = pd.read_csv('./data/FY22 Performance of Sales Reps - Sample Dataset v2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBREGION', 'COUNTRY', 'CAREER_LEVEL', 'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN', 'TENURE_LATEST_HIRE_DATE_BAND_LIN', 'JOB_TENURE_BAND_LIN', 'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER', 'AGE_BAND', 'PRODUCT_LINE', 'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07', 'NATIONALITY', 'MANAGER_GENDER_DESC', 'HIRE_EVENT_DESCRIPTION', 'NUM_SALARY_CHANGE_PREV_FY', 'NUM_SALARY_CHANGE_CURR_FY', 'NUM_ORG_CHANGE_PREV_FY', 'NUM_ORG_CHANGE_CURR_FY', 'NUM_CAREER_LEVEL_CHANGE_PREV_FY', 'NUM_CAREER_LEVEL_CHANGE_CURR_FY', 'NUM_MANAGER_CHANGE_PREV_FY', 'NUM_MANAGER_CHANGE_CURR_FY', 'NUM_JOB_CHANGE_PREV_FY', 'NUM_JOB_CHANGE_CURR_FY', 'NUM_LOCATION_CHANGE_PREV_FY', 'NUM_LOCATION_CHANGE_CURR_FY', 'MANAGER180_OVERALL_BAND_CURR_FY', 'ATTAINMENT_REP_LEVEL_BAND_PREV_FY', 'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'], dtype='object')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBREGION', 'COUNTRY', 'CAREER_LEVEL', 'TENURE_CONTINUOUS_SERVICE_DATE_BAND_LIN', 'TENURE_LATEST_HIRE_DATE_BAND_LIN', 'JOB_TENURE_BAND_LIN', 'TIME_SINCE_LAST_SALARY_INCR_BAND', 'GENDER', 'AGE_BAND', 'PRODUCT_LINE', 'PRODUCT_ASSOCIATION', 'RCODE_06', 'RCODE_07', 'NATIONALITY', 'MANAGER_GENDER_DESC', 'HIRE_EVENT_DESCRIPTION', 'NUM_SALARY_CHANGE_PREV_FY', 'NUM_SALARY_CHANGE_CURR_FY', 'NUM_ORG_CHANGE_PREV_FY', 'NUM_ORG_CHANGE_CURR_FY', 'NUM_CAREER_LEVEL_CHANGE_PREV_FY', 'NUM_CAREER_LEVEL_CHANGE_CURR_FY', 'NUM_MANAGER_CHANGE_PREV_FY', 'NUM_MANAGER_CHANGE_CURR_FY', 'NUM_JOB_CHANGE_PREV_FY', 'NUM_JOB_CHANGE_CURR_FY', 'NUM_LOCATION_CHANGE_PREV_FY', 'NUM_LOCATION_CHANGE_CURR_FY', 'MANAGER180_OVERALL_BAND_CURR_FY', 'ATTAINMENT_REP_LEVEL_BAND_PREV_FY', 'MANAGER_JOB_LEVEL_CATEGORY_REPLEVEL'], dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_model - prediction on new data\n",
    "lr2_pred_new = lr_model2.predict(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_model - prediction on new data\n",
    "rf_pred_new = rf_model.predict(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm_model - prediction on new data\n",
    "lgbm_pred_new = lgbm_model.predict(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc_model - prediction on new data\n",
    "svc_pred_new = svc_model.predict(df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 923us/step\n"
     ]
    }
   ],
   "source": [
    "#nn_model - prediction on new data\n",
    "nn_pred_new = rscv_nn.best_estimator_.predict(df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the results of the predictions derived from the 5 models into a dataframe\n",
    "df_predict_classes = pd.DataFrame(data=[lr2_pred_new, rf_pred_new, lgbm_pred_new, svc_pred_new, nn_pred_new]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1362 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3    4\n",
       "0     1  0  1  1  [1]\n",
       "1     0  0  0  0  [0]\n",
       "2     1  0  0  1  [0]\n",
       "3     1  0  0  0  [0]\n",
       "4     0  0  0  0  [0]\n",
       "...  .. .. .. ..  ...\n",
       "1357  1  1  0  0  [0]\n",
       "1358  0  1  0  0  [0]\n",
       "1359  1  0  0  1  [0]\n",
       "1360  1  1  1  1  [1]\n",
       "1361  0  0  0  0  [0]\n",
       "\n",
       "[1362 rows x 5 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dataframe\n",
    "df_predict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the column names\n",
    "df_predict_classes.columns = ['LOGISTIC_REGRESSION','RANDOM_FOREST','LIGHTGBM','SUPPORT_VECTOR','SHALLOW_NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the brackets of the NN column\n",
    "df_predict_classes= df_predict_classes.explode('SHALLOW_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOGISTIC_REGRESSION</th>\n",
       "      <th>RANDOM_FOREST</th>\n",
       "      <th>LIGHTGBM</th>\n",
       "      <th>SUPPORT_VECTOR</th>\n",
       "      <th>SHALLOW_NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1362 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LOGISTIC_REGRESSION RANDOM_FOREST LIGHTGBM SUPPORT_VECTOR SHALLOW_NN\n",
       "0                      1             0        1              1          1\n",
       "1                      0             0        0              0          0\n",
       "2                      1             0        0              1          0\n",
       "3                      1             0        0              0          0\n",
       "4                      0             0        0              0          0\n",
       "...                  ...           ...      ...            ...        ...\n",
       "1357                   1             1        0              0          0\n",
       "1358                   0             1        0              0          0\n",
       "1359                   1             0        0              1          0\n",
       "1360                   1             1        1              1          1\n",
       "1361                   0             0        0              0          0\n",
       "\n",
       "[1362 rows x 5 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dataframe\n",
    "df_predict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the mode to get the most commonly predicted class. \n",
    "#since there are 5 models, there will not be any tie\n",
    "\n",
    "df_predict_classes['MAX_VOTE'] = df_predict_classes.mode(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the values to string to make it easier for interpretation\n",
    "d = {'0': 'Not Good', '1' :'Good'}\n",
    "cols = ['LOGISTIC_REGRESSION','RANDOM_FOREST','LIGHTGBM','SUPPORT_VECTOR','SHALLOW_NN','MAX_VOTE']\n",
    "df_predict_classes[cols] = df_predict_classes[cols].replace(d, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOGISTIC_REGRESSION</th>\n",
       "      <th>RANDOM_FOREST</th>\n",
       "      <th>LIGHTGBM</th>\n",
       "      <th>SUPPORT_VECTOR</th>\n",
       "      <th>SHALLOW_NN</th>\n",
       "      <th>MAX_VOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>Not Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>Not Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1362 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LOGISTIC_REGRESSION RANDOM_FOREST  LIGHTGBM SUPPORT_VECTOR SHALLOW_NN  MAX_VOTE\n",
       "0                   Good      Not Good      Good           Good       Good      Good\n",
       "1               Not Good      Not Good  Not Good       Not Good   Not Good  Not Good\n",
       "2                   Good      Not Good  Not Good           Good   Not Good  Not Good\n",
       "3                   Good      Not Good  Not Good       Not Good   Not Good  Not Good\n",
       "4               Not Good      Not Good  Not Good       Not Good   Not Good  Not Good\n",
       "...                  ...           ...       ...            ...        ...       ...\n",
       "1357                Good          Good  Not Good       Not Good   Not Good  Not Good\n",
       "1358            Not Good          Good  Not Good       Not Good   Not Good  Not Good\n",
       "1359                Good      Not Good  Not Good           Good   Not Good  Not Good\n",
       "1360                Good          Good      Good           Good       Good      Good\n",
       "1361            Not Good      Not Good  Not Good       Not Good   Not Good  Not Good\n",
       "\n",
       "[1362 rows x 6 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge predictions with original dataframe of the new data\n",
    "df_out = pd.merge(df_new,df_predict_classes,how = 'left',left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "#df_out.to_csv('./data/Predictions for FY22 Sales Rep Performance.csv', encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary: Model Evaluation using ROC AUC (optional to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: Generally, the use of ROC curves and precision-recall curves are as follows:**</font>\n",
    "\n",
    "<font size=\"3\">**ROC curves should be used when there are roughly equal numbers of observations for each class.\n",
    "Precision-Recall curves should be used when there is a moderate to large class imbalance.\n",
    "The reason for this recommendation is that ROC curves can present an overly optimistic view of an algorithms performance if there is a large skew in the class distribution**</font>\n",
    "\n",
    "<font size=\"3\">**Precision-Recall (PR) curves, often used in Information Retrieval , have been cited as an alternative to ROC curves for tasks with a large skew in the class distribution.**</font>\n",
    "\n",
    "from https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\">**Note: in y_test, 'Not Good' was coded as '0' and 'Good' was coded as '1'**</font>\n",
    "\n",
    "<font size=\"2\">**check y_test index and df2 index for confirmation**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the auc for random forest\n",
    "#in this case, the [:,1] is used to predict classes that are 'Good'\n",
    "auc1 = metrics.roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412393162393164"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value counts\n",
    "#y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y values have to binarized first\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "dummy_y_val = lb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Note: [:,1] means predicting the probabilities for positive class - which are the good performers**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the probabilities of the validation data for positive class\n",
    "#use metrics.roc_curve function to calculate the false positive rate and true positive rate\n",
    "#use the values to get the auc score\n",
    "\n",
    "y_pred_proba_lr = lr_model2.predict_proba(X_val)[:,1]\n",
    "fpr1, tpr1, _ = metrics.roc_curve(dummy_y_val, y_pred_proba_lr, pos_label=1)\n",
    "auc1 = metrics.roc_auc_score(dummy_y_val, y_pred_proba_lr)\n",
    "\n",
    "\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_val)[:,1]\n",
    "fpr2, tpr2, _ = metrics.roc_curve(dummy_y_val, y_pred_proba_rf, pos_label=1)\n",
    "auc2 = metrics.roc_auc_score(dummy_y_val, y_pred_proba_rf)\n",
    "\n",
    "y_pred_proba_lgbm = lgbm_model.predict_proba(X_val)[:,1]\n",
    "fpr3, tpr3, _ = metrics.roc_curve(dummy_y_val, y_pred_proba_lgbm, pos_label=1)\n",
    "auc3 = metrics.roc_auc_score(dummy_y_val, y_pred_proba_lgbm)\n",
    "\n",
    "\n",
    "y_pred_proba_svc = svc_model.predict_proba(X_val)[:,1]\n",
    "fpr4, tpr4, _ = metrics.roc_curve(dummy_y_val, y_pred_proba_svc, pos_label=1)\n",
    "auc4 = metrics.roc_auc_score(dummy_y_val, y_pred_proba_svc)\n",
    "\n",
    "# y_pred_proba_nn = nn_model_best.predict_proba(X_test)[::,1]\n",
    "# fpr4, tpr4, _ = metrics.roc_curve(y_test2, y_pred_proba_nn)\n",
    "# auc4 = metrics.roc_auc_score(y_test2, y_pred_proba_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the AUC curve for all 4 algorithms\n",
    "plt.figure(figsize =(10,10))\n",
    "\n",
    "\n",
    "plt.plot(fpr1, tpr1, label= \"Logistic Regression, auc=\" +str(round(auc1,2)))\n",
    "plt.plot(fpr2, tpr2, label= \"Random Forest, auc=\" +str(round(auc2,2)))\n",
    "plt.plot(fpr3, tpr3, label= \"Light GBM, auc=\" +str(round(auc3,2)))\n",
    "plt.plot(fpr4, tpr4, label= \"Support Vector Machine, auc=\" +str(round(auc4,2)))\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "\n",
    "plt.legend(loc=4, title = 'Models', facecolor='white')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.title('Receiver Operating Characteristic', size = 12)\n",
    "plt.box(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
